{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLGv1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TM-tTiZ8g3YUuXgleiYK40hwbNyoLhEy",
      "authorship_tag": "ABX9TyNIYjhb7gWM2bJBlfu04PwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashantmishra311/Attentive-Seq2Seq-with-Copying-Mechanism-for-Table-to-text/blob/master/NLGv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VzOmKUsbl-a"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import regex as re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "f5KVOBvWdJdT",
        "outputId": "ecf27f85-fb7e-4802-f7f2-c568de626871"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Wikitable_data_splits_Batch_model/train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attributes</th>\n",
              "      <th>cells</th>\n",
              "      <th>captions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>subj_title subj_subtitle date winning_$$_team ...</td>\n",
              "      <td>1978_$$_federation_$$_cup_$$_(_$$_tennis_$$_) ...</td>\n",
              "      <td>philippines won thailand with 3–0 during 1978 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>subj_title subj_subtitle playoff_$$_round date...</td>\n",
              "      <td>1985_$$_new_$$_england_$$_patriots_$$_season s...</td>\n",
              "      <td>afc_$$_championship was played on january_$$_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>subj_title subj_subtitle game date opponent re...</td>\n",
              "      <td>1961_$$_minnesota_$$_vikings_$$_season preseas...</td>\n",
              "      <td>minnesota vikings season was in the memorial_$...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>subj_title subj_subtitle position name term_$$...</td>\n",
              "      <td>st_$$_._$$_augustine_$$_beach_$$_,_$$_florida ...</td>\n",
              "      <td>undine_$$_pawlowski_$$_george was vice-mayor o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>subj_title subj_subtitle time name height weig...</td>\n",
              "      <td>40-yard_$$_dash records 4.30 darrius_$$_heywar...</td>\n",
              "      <td>darrius_$$_heyward-bey was 6_$$_ft_$$_2_$$_in_...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          attributes  ...                                           captions\n",
              "0  subj_title subj_subtitle date winning_$$_team ...  ...  philippines won thailand with 3–0 during 1978 ...\n",
              "1  subj_title subj_subtitle playoff_$$_round date...  ...  afc_$$_championship was played on january_$$_1...\n",
              "2  subj_title subj_subtitle game date opponent re...  ...  minnesota vikings season was in the memorial_$...\n",
              "3  subj_title subj_subtitle position name term_$$...  ...  undine_$$_pawlowski_$$_george was vice-mayor o...\n",
              "4  subj_title subj_subtitle time name height weig...  ...  darrius_$$_heyward-bey was 6_$$_ft_$$_2_$$_in_...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhwpvxFElKSt"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    Arguments:-\n",
        "        vocab_size: (int) Size of source vocabulary\n",
        "        embedding_size: (int) Embedding size\n",
        "        encoder_size: (int) Dimensions of encoder hidden state\n",
        "        batch_size: (int) Batch size\n",
        "    '''\n",
        "\n",
        "    def __init__(self, attr_vocab_size, attr_embedding_size, vocab_size, embedding_size, encoder_size, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.encoder_size = encoder_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.attr_vocab_size = attr_vocab_size\n",
        "        self.attr_embedding_size = attr_embedding_size\n",
        "\n",
        "        # attr_embedding_input_shape = (batch_size, seq_length)\n",
        "        self.Attr_Embedding = tf.keras.layers.Embedding(self.attr_vocab_size, self.attr_embedding_size)\n",
        "        # attr_embedding_output_shape = (batch_size, seq_length, attr_embedding_size)\n",
        "\n",
        "        # embedding_input_shape = (batch_size, seq_length)\n",
        "        self.Embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        # embedding_output_shape = (batch_size, seq_length, embedding_size)\n",
        "\n",
        "        self.W_e = tf.keras.layers.Dense(self.embedding_size)\n",
        "\n",
        "        # gru_input_shape = (batch_size, seq_length, embedding_size)\n",
        "        self.GRU = tf.keras.layers.GRU(self.encoder_size, \n",
        "                                       return_sequences=True, \n",
        "                                       return_state=True)\n",
        "        # output_shape = (batch_size, seq_length, encoder_size)\n",
        "        # hidden_state_shape = (batch_size, encoder_size)\n",
        "    \n",
        "    def call(self, attr_input, input, prev_hidden_state):\n",
        "\n",
        "        # attr_input --> (batch_size, seq_length)\n",
        "        attr_embed_output = self.Attr_Embedding(attr_input)\n",
        "        # attr_embed_output --> (batch_size, seq_length, attr_embedding_size)\n",
        "        \n",
        "        # input --> (batch_size, seq_length)\n",
        "        embed_output = self.Embedding(input)\n",
        "        # embed_output --> (batch_size, seq_length, embedding_size)\n",
        "\n",
        "        concat_embeds = tf.concat([embed_output, attr_embed_output], axis=2)\n",
        "        # concat_embeds --> (batch_size, seq_length, embedding_size+attr_embedding_size)\n",
        "\n",
        "        embed_output_ = tf.nn.tanh(self.W_e(concat_embeds))\n",
        "\n",
        "        output, hidden_state = self.GRU(embed_output_, initial_state=prev_hidden_state)\n",
        "        # output --> (batch_size, seq_length, encoder_size)\n",
        "        # hidden_state --> (batch_size, encoder_size)\n",
        "        return output, hidden_state\n",
        "\n",
        "    def init_hidden_state(self):\n",
        "        init_hidden = tf.zeros([self.batch_size, self.encoder_size])\n",
        "        # init_hidden --> (batch_size, encoder_size)\n",
        "        return init_hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrl--Sj-H8Ps"
      },
      "source": [
        "* **Luong Score:** $$ \\text{score}(h_t,\\bar{h_s}) = h_t^\\top \\mathbf{W} \\bar{h_s}$$\n",
        "* **Bahdanau Score:**$$ \\text{score}(h_t,\\bar{h_s}) = \\nu^\\top \\text{tanh}(\\mathbf{W_1}h_t +\\mathbf{W_2} \\bar{h_s}) $$\n",
        "* **Attention Weights:** $$\\alpha_{ts} = \\frac{\\exp\\{\\text{score}(h_t,\\bar{h_s})\\}}{\\sum _{i=1}^S \\exp\\{\\text{score}(h_t,\\bar{h_i})\\}} $$\n",
        "* **Context Vector:** $$ \\mathbf{c_t} = \\sum _{s} \\alpha_{ts}\\bar{h_s} $$\n",
        "* **Attention Vector:** $$ \\mathbf{a_t} = \\text{tanh}(\\mathbf{W_c}[\\mathbf{c_t};h_t]) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3MihoBA4y2r"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "\n",
        "    '''\n",
        "    Arguments:-\n",
        "        attention_size: (int) Must be same as decoder_size\n",
        "        style: attention mechanism, 'bahdanau' or 'luong'\n",
        "    '''\n",
        "\n",
        "    def __init__(self, attention_size, style='bahdanau'):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        if style.lower() not in ['bahdanau', 'luong']:\n",
        "            raise ValueError(f'Attention style {style} unrecognized, try \"bahdanau\" or \"luong\"')\n",
        "\n",
        "        self.attention_size = attention_size\n",
        "        self.style = style.lower()\n",
        "\n",
        "        # for 'Bahdanau' style attention\n",
        "        self.W1 = tf.keras.layers.Dense(self.attention_size) # --> (, attention_size)\n",
        "        self.W2 = tf.keras.layers.Dense(self.attention_size) # --> (, attention_size)\n",
        "        self.v = tf.keras.layers.Dense(1) # --> (, 1)\n",
        "\n",
        "        # for 'Luong' style attention\n",
        "        self.W = tf.keras.layers.Dense(self.attention_size)\n",
        "    \n",
        "    def call(self, decoder_current_hidden, encoder_output):\n",
        "        \n",
        "        # encoder_output --> (batch_size, seq_length, encoder_size)\n",
        "        # decoder_current_hidden --> (batch_size, decoder_size)\n",
        "        decoder_current_hidden = tf.expand_dims(decoder_current_hidden, axis=1) \n",
        "        # decoder_current_hidden --> (batch_size, 1, decoder_size)\n",
        "\n",
        "        bahdanau_score = self.v(tf.nn.tanh(self.W1(decoder_current_hidden) + self.W2(encoder_output)))\n",
        "        # bahdanau_score --> (1)'((batch_size, 1, attention_size) + (batch_size, seq_length, attention_size))\n",
        "        # bahdanau_score --> (1)'(batch_size, seq_length, attention_size) [broadcasting over axis 1]\n",
        "        # bahdanau_score --> (batch_size, seq_length, 1)\n",
        "\n",
        "        luong_score = tf.matmul(self.W(encoder_output), decoder_current_hidden, transpose_b=True)\n",
        "        # luong_score --> ((batch_size, seq_length, attention_size), (batch_size, 1, decoder_size)')\n",
        "        # luong_score --> (batch_size, seq_length, 1) [attention_size must be equal to decoder_size]\n",
        "\n",
        "        score = bahdanau_score if self.style == 'bahdanau' else luong_score\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        # attention_weights --> (batch_size, seq_length, 1) [floats between 0 and 1]\n",
        "\n",
        "        context_vector = tf.reduce_sum(attention_weights*encoder_output, axis=1)\n",
        "        # attention_weights*encoder_output --> (batch_size, seq_length, encoder_size)\n",
        "        # context_vector --> (batch_size, encoder_size)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "246yfEypqAos"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    Arguments:-\n",
        "        vocab_size: (int) Size of target vocabulary\n",
        "        embedding_size: (int) Embedding size\n",
        "        decoder_size: (int) Dimensions of decoder hidden state\n",
        "        batch_size: (int) Batch size\n",
        "        attention_style: attention mechanism, 'bahdanau' or 'luong'\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_size, decoder_size, batch_size, attention_style='bahdanau'):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.decoder_size = decoder_size\n",
        "        self.batch_size = batch_size\n",
        "        self.style = attention_style\n",
        "\n",
        "        # embedding_input_shape = (batch_size, 1) [seq_length = 1 for decoder]\n",
        "        self.Embedding = tf.keras.layers.Embedding(self.vocab_size, self.embedding_size)\n",
        "        # embedding_output_shape = (batch_size, 1, embedding_size)\n",
        "\n",
        "        # gru_input_shape = (batch_size, 1, embedding_size+...)\n",
        "        self.GRU = tf.keras.layers.GRU(self.decoder_size, \n",
        "                                       return_sequences=True, \n",
        "                                       return_state=True)\n",
        "        # output_shape = (batch_size, 1, decoder_size)\n",
        "        # hidden_state_shape = (batch_size, decoder_size)\n",
        "\n",
        "        self.Linear = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "        self.attention = Attention(self.decoder_size, style=self.style)\n",
        "        # --------------with Pgen--------------------------\n",
        "        self.wh = tf.keras.layers.Dense(1)\n",
        "        self.ws = tf.keras.layers.Dense(1)\n",
        "        self.wx = tf.keras.layers.Dense(1)\n",
        "        self.Pgen = tf.keras.layers.Dense(1, use_bias=True)\n",
        "\n",
        "    def call(self, input, decoder_prev_hidden, encoder_output):\n",
        "\n",
        "        # input --> (batch_size, 1) [seq_length = 1 for decoder]\n",
        "        embed_output = self.Embedding(input)\n",
        "        # embed_output --> (batch_size, 1, embedding_size)\n",
        "\n",
        "        # encoder_output --> (batch_size, seq_length, encoder_size)\n",
        "        # decoder_prev_hidden --> (batch_size, decoder_size)\n",
        "        context_vector, attention_weights = self.attention(decoder_prev_hidden, encoder_output)\n",
        "        # context_vector --> (batch_size, encoder_size)\n",
        "        # attention_weights --> (batch_size, seq_length, 1)\n",
        "\n",
        "        '''# --------------without Pgen-----------------------\n",
        "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
        "        # context_vector --> (batch_size, 1, encoder_size)\n",
        "\n",
        "        gru_input = tf.concat([context_vector, embed_output], axis=2)\n",
        "        # gru_input --> (batch_size, 1, encoder_size+embedding_size)\n",
        "\n",
        "        output, hidden_state = self.GRU(gru_input)\n",
        "        # output --> (batch_size, 1, decoder_size)\n",
        "        # hidden_state --> (batch_size, decoder_size)\n",
        "\n",
        "        output = tf.reshape(output, [output.shape[0], output.shape[2]])\n",
        "        # output --> (batch_size, decoder_size)\n",
        "        output = self.Linear(output)\n",
        "        # output --> (batch_size, vocab_size)\n",
        "        return output, hidden_state, attention_weights'''\n",
        "\n",
        "        # --------------with Pgen--------------------------\n",
        "        context_vector = context_vector\n",
        "        # context_vector --> (batch_size, encoder_size)\n",
        "        decoder_state = decoder_prev_hidden\n",
        "        # decoder_state --> (batch_size, decoder_size)\n",
        "        decoder_input = tf.reshape(embed_output, [embed_output.shape[0], embed_output.shape[2]])\n",
        "        # decoder_input = (batch_size, embedding_size)\n",
        "\n",
        "        concat_ = tf.concat([self.ws(decoder_state),self.wx(decoder_input)], axis=1)\n",
        "        concat = tf.concat([self.wh(context_vector),concat_], axis=1)\n",
        "        # concat --> (batch_size, 3)\n",
        "        p_gen = tf.nn.sigmoid(self.Pgen(concat))\n",
        "        # p_gen --> (batch_size, 1)\n",
        "\n",
        "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
        "        # context_vector --> (batch_size, 1, encoder_size)\n",
        "\n",
        "        gru_input = tf.concat([context_vector, embed_output], axis=2)\n",
        "        # gru_input --> (batch_size, 1, encoder_size+embedding_size)\n",
        "\n",
        "        output, hidden_state = self.GRU(gru_input, initial_state=decoder_prev_hidden)\n",
        "        # output --> (batch_size, 1, decoder_size)\n",
        "        # hidden_state --> (batch_size, decoder_size)\n",
        "\n",
        "        output = tf.reshape(output, [output.shape[0], output.shape[2]])\n",
        "        # output --> (batch_size, decoder_size)\n",
        "        outputs = tf.nn.log_softmax(self.Linear(output))\n",
        "        # outputs --> (batch_size, vocab_size)\n",
        "        return outputs, hidden_state, attention_weights, p_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRYWnmX5uYif"
      },
      "source": [
        "def final_dist(vocab_dist, attn_dist, max_enc_seq_len):\n",
        "    \"\"\"Calculate the final distribution, for the pointer-generator model\n",
        "    Args:\n",
        "      vocab_dist: The vocabulary distributions. (batch_size, vsize) arrays\n",
        "      attn_dist: The attention distributions. (batch_size, attn_len) arrays\n",
        "    Returns:\n",
        "      final_dist: The final distributions. (batch_size, extended_vsize) arrays.\n",
        "    \"\"\"\n",
        "    vocab_dist_ = p_gen*vocab_dist\n",
        "    attn_dist_ = (1-p_gen)*attn_dist\n",
        "\n",
        "    batchSize = vocab_dist.shape[0]\n",
        "    decVocabSize = vocab_dist.shape[1]\n",
        "    ExtdecVocabSize = decVocabSize + max_enc_seq_len\n",
        "    extra_zeros = tf.zeros((batchSize, max_enc_seq_len))\n",
        "    vocab_dist_extended = tf.concat((vocab_dist_,extra_zeros), axis=1)\n",
        "\n",
        "    batch_nums = tf.range(0, limit=batchSize) # shape (batch_size)\n",
        "    batch_nums = tf.expand_dims(batch_nums, 1) # shape (batch_size, 1)\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-evMnYTaYOIA"
      },
      "source": [
        "def preprocess(text, lower=True, match_sub={('[^A-Za-z0-9]',' '),\n",
        "                                            ('\\s+',' '),\n",
        "                                            ('\\s+$','')}):\n",
        "    text = text.lower() if lower is True else text\n",
        "    for (pattern,sub) in match_sub:\n",
        "        text = re.sub(pattern, sub, text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-95VuQTDD6N4"
      },
      "source": [
        "df.captions = df.captions.apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqT4_TU03zeQ"
      },
      "source": [
        "df.captions = df.captions.apply(lambda text: '<start> ' + text + ' <end>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh6uIIfN_51R"
      },
      "source": [
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FkfpzINAYnb"
      },
      "source": [
        "tokenizer.fit_on_texts(df['attributes'])\n",
        "tokenizer.fit_on_texts(df['cells'])\n",
        "tokenizer.fit_on_texts(df['captions'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvjB28ByAjfZ"
      },
      "source": [
        "with open('vocab.txt', 'w') as f:\n",
        "    for (word, count) in tokenizer.word_counts.items():\n",
        "        f.write(f'{word} {count}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTHGbKbbECX7"
      },
      "source": [
        "msk = np.random.rand(len(df)) < 0.8\n",
        "df_tr = df[msk]\n",
        "df_te = df[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaB20lOs48Gw"
      },
      "source": [
        "att_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "att_tokenizer.fit_on_texts(df_tr['attributes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVHrhSb0FAD6"
      },
      "source": [
        "src_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "src_tokenizer.fit_on_texts(df_tr['cells'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUSfav8INRha"
      },
      "source": [
        "trg_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "trg_tokenizer.fit_on_texts(df_tr['captions'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMolSU3NlkL"
      },
      "source": [
        "att_inp_tr = att_tokenizer.texts_to_sequences(df_tr['attributes'])\n",
        "att_inp_tr = tf.keras.preprocessing.sequence.pad_sequences(att_inp_tr, padding='post')\n",
        "\n",
        "src_inp_tr = src_tokenizer.texts_to_sequences(df_tr['cells'])\n",
        "src_inp_tr = tf.keras.preprocessing.sequence.pad_sequences(src_inp_tr, padding='post')\n",
        "\n",
        "trg_out_tr = trg_tokenizer.texts_to_sequences(df_tr['captions'])\n",
        "trg_out_tr = tf.keras.preprocessing.sequence.pad_sequences(trg_out_tr, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10GKubtAPc5W"
      },
      "source": [
        "att_inp_te = att_tokenizer.texts_to_sequences(df_te['attributes'])\n",
        "att_inp_te = tf.keras.preprocessing.sequence.pad_sequences(att_inp_te, padding='post', maxlen=att_inp_tr.shape[1])\n",
        "\n",
        "src_inp_te = src_tokenizer.texts_to_sequences(df_te['cells'])\n",
        "src_inp_te = tf.keras.preprocessing.sequence.pad_sequences(src_inp_te, padding='post', maxlen=src_inp_tr.shape[1])\n",
        "\n",
        "trg_out_te = trg_tokenizer.texts_to_sequences(df_te['captions'])\n",
        "trg_out_te = tf.keras.preprocessing.sequence.pad_sequences(trg_out_te, padding='post', maxlen=trg_out_tr.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XME1zTEyGcZ5"
      },
      "source": [
        "att_inp_tr = tokenizer.texts_to_sequences(df_tr['attributes'])\n",
        "att_inp_tr = tf.keras.preprocessing.sequence.pad_sequences(att_inp_tr, padding='post')\n",
        "\n",
        "src_inp_tr = tokenizer.texts_to_sequences(df_tr['cells'])\n",
        "src_inp_tr = tf.keras.preprocessing.sequence.pad_sequences(src_inp_tr, padding='post')\n",
        "\n",
        "trg_out_tr = tokenizer.texts_to_sequences(df_tr['captions'])\n",
        "trg_out_tr = tf.keras.preprocessing.sequence.pad_sequences(trg_out_tr, padding='post')\n",
        "\n",
        "att_inp_te = tokenizer.texts_to_sequences(df_te['attributes'])\n",
        "att_inp_te = tf.keras.preprocessing.sequence.pad_sequences(att_inp_te, padding='post', maxlen=att_inp_tr.shape[1])\n",
        "\n",
        "src_inp_te = tokenizer.texts_to_sequences(df_te['cells'])\n",
        "src_inp_te = tf.keras.preprocessing.sequence.pad_sequences(src_inp_te, padding='post', maxlen=src_inp_tr.shape[1])\n",
        "\n",
        "trg_out_te = tokenizer.texts_to_sequences(df_te['captions'])\n",
        "trg_out_te = tf.keras.preprocessing.sequence.pad_sequences(trg_out_te, padding='post', maxlen=trg_out_tr.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4jLP9HRTgWz"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfunBeiNuxbR"
      },
      "source": [
        "@tf.function\n",
        "def train_step(att_inp, inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(att_inp, inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1) # <- trg_tokenizer\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHCQBJ70kM6_"
      },
      "source": [
        "def val_step(att_inp, inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    enc_output, enc_hidden = encoder(att_inp, inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1) # <- trg_tokenizer\n",
        "\n",
        "    # No Teacher forcing - feeding the prediction as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "        loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "        # not using teacher forcing\n",
        "        predicted_id = tf.argmax(predictions, axis=1)\n",
        "\n",
        "        dec_input = tf.expand_dims(predicted_id, 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr0V9EcCUE-N"
      },
      "source": [
        "BUFFER_SIZE = 8\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "dataset_tr = tf.data.Dataset.from_tensor_slices((att_inp_tr, src_inp_tr, trg_out_tr))\n",
        "dataset_tr = dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL56i75tng-3"
      },
      "source": [
        "BUFFER_SIZE_ = 8\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "dataset_te = tf.data.Dataset.from_tensor_slices((att_inp_te, src_inp_te, trg_out_te))\n",
        "dataset_te = dataset_te.shuffle(BUFFER_SIZE_).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_lMlJucX-pq"
      },
      "source": [
        "vocab_att_size = len(tokenizer.word_index)+1 # <- att_\n",
        "vocab_inp_size = len(tokenizer.word_index)+1\n",
        "vocab_tar_size = len(tokenizer.word_index)+1\n",
        "\n",
        "EMBEDDING_SIZE_en = 16\n",
        "EMBEDDING_SIZE_de = 32\n",
        "\n",
        "ENC_SIZE = 64\n",
        "DEC_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vybX_Y54ZYJp"
      },
      "source": [
        "encoder = Encoder(vocab_att_size, EMBEDDING_SIZE_en, vocab_inp_size, EMBEDDING_SIZE_en, ENC_SIZE, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, EMBEDDING_SIZE_de, DEC_SIZE, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIb3flQrI1z_"
      },
      "source": [
        "attr_te = df_te.attributes.to_list()\n",
        "cell_te = df_te.cells.to_list()\n",
        "capt_te = df_te.captions.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQh6WCWEdLJO"
      },
      "source": [
        "def predict(text=None):\n",
        "    \n",
        "    '''if text is not None:\n",
        "        test_source_text = text\n",
        "        print('Source: ',test_source_text)\n",
        "    else:\n",
        "        test_source_text = np.random.choice(cell_te)\n",
        "        print('Source: ',test_source_text)\n",
        "        idx = cell_te.index(test_source_text)\n",
        "        print('Target: ',capt_te[idx])\n",
        "\n",
        "    test_attr_seq = att_tokenizer.texts_to_sequences([attr_te[idx]])\n",
        "    test_source_seq = src_tokenizer.texts_to_sequences([test_source_text])'''\n",
        "    #print(test_source_seq)\n",
        "    att, cell, capt = next(iter(dataset_te.take(1)))\n",
        "    x = np.random.randint(low=0, high=BATCH_SIZE)\n",
        "    cell_vals = [tokenizer.index_word[idx] for idx in cell[x].numpy() if idx != 0] # <- src_tokenizer\n",
        "    capt_vals = [tokenizer.index_word[idx] for idx in capt[x].numpy() if idx != 0] # <- trg_tokenizer\n",
        "    print('Source: ',' '.join(cell_vals))\n",
        "    print('Target: ',' '.join(capt_vals))\n",
        "\n",
        "    en_initial_states = tf.zeros([1, encoder.encoder_size])\n",
        "    enc_output, enc_hidden = encoder(tf.expand_dims(att[x],0), tf.expand_dims(cell[x],0), en_initial_states)\n",
        "\n",
        "    dec_input = tf.constant([[tokenizer.word_index['<start>']]]) # <- trg_tokenizer\n",
        "    dec_hidden = enc_hidden\n",
        "    out_words = ['<start>']\n",
        "\n",
        "    while True:\n",
        "        predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        dec_input = tf.expand_dims(tf.argmax(predictions, -1), 0)\n",
        "        out_words.append(tokenizer.index_word[dec_input.numpy()[0][0]]) # <- trg_tokenizer\n",
        "\n",
        "        if len(out_words) >= 12: # out_words[-1] == '<end>' or\n",
        "            break\n",
        "\n",
        "    print('Predic: ',' '.join(out_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAms8gyuWrMP"
      },
      "source": [
        "def run(epochs, print_per_epoch, steps_per_epoch, steps_per_epoch_):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        enc_hidden = encoder.init_hidden_state()\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (att, inp, targ)) in enumerate(dataset_tr.take(steps_per_epoch)):\n",
        "            batch_loss = train_step(att, inp, targ, enc_hidden)\n",
        "            total_loss += batch_loss\n",
        "        train_loss = total_loss / steps_per_epoch\n",
        "        \n",
        "        val_loss = 0\n",
        "        for (batch, (att, inp, targ)) in enumerate(dataset_te.take(steps_per_epoch_)):\n",
        "            batch_loss = val_step(att, inp, targ, enc_hidden)\n",
        "            val_loss += batch_loss\n",
        "        valid_loss = val_loss / steps_per_epoch_\n",
        "\n",
        "        '''if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))'''\n",
        "\n",
        "        # saving (checkpoint) the model every 2 epochs\n",
        "        # if (epoch + 1) % 2 == 0:\n",
        "        #  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        if epoch % print_per_epoch == 0:\n",
        "            print('----------------------<><><><><>-----------------------')\n",
        "            print('Epoch {} | Train Loss {:.4f} | Val Loss {:.4f}'.format(epoch + 1, train_loss, valid_loss))\n",
        "            # try:\n",
        "            predict()\n",
        "            # except Exception:\n",
        "                #continue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0-QrQ7-Bz2R"
      },
      "source": [
        "EPOCHS = 400\n",
        "STEPS_PER_EPOCH = BUFFER_SIZE//BATCH_SIZE\n",
        "STEPS_PER_EPOCH_ = BUFFER_SIZE_//BATCH_SIZE\n",
        "PRINT_PER_EPOCH = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R_TBz8YCFLs",
        "outputId": "69ee50fe-653c-468f-eb12-dca2a1929f8e"
      },
      "source": [
        "run(EPOCHS, PRINT_PER_EPOCH, STEPS_PER_EPOCH, STEPS_PER_EPOCH_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------<><><><><>-----------------------\n",
            "Epoch 1 | Train Loss 1.9665 | Val Loss 3.1529\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> william was was was and <end> <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 6 | Train Loss 1.9822 | Val Loss 2.6120\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> william was was was and and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 11 | Train Loss 2.1949 | Val Loss 2.8263\n",
            "Source:  christopher female 58 73 68\n",
            "Target:  <start> christopher scored average marks and she can work more on her math <end>\n",
            "Predic:  <start> oliver was was was and and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 16 | Train Loss 2.2518 | Val Loss 2.8090\n",
            "Source:  alexander male 58 54 52\n",
            "Target:  <start> alexander was average but he was consistent <end>\n",
            "Predic:  <start> oliver was was was the and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 21 | Train Loss 2.1428 | Val Loss 2.6298\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> oliver was was was and and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 26 | Train Loss 2.1184 | Val Loss 2.9548\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> oliver was was was and and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 31 | Train Loss 2.0369 | Val Loss 2.7350\n",
            "Source:  noah female 69 90 88\n",
            "Target:  <start> noah scored good in reading and writing but her math score was an anomaly <end>\n",
            "Predic:  <start> oliver was was was and and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 36 | Train Loss 2.0742 | Val Loss 2.9376\n",
            "Source:  andrew female 53 58 65\n",
            "Target:  <start> andrew scored average marks and he can work harder on his math and reading <end>\n",
            "Predic:  <start> james was was was and and <end> <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 41 | Train Loss 2.0436 | Val Loss 2.8445\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> james was was was and and and <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 46 | Train Loss 2.3454 | Val Loss 3.0494\n",
            "Source:  christopher female 58 73 68\n",
            "Target:  <start> christopher scored average marks and she can work more on her math <end>\n",
            "Predic:  <start> james was was was and and and <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 51 | Train Loss 1.7160 | Val Loss 3.0047\n",
            "Source:  robert female 58 63 73\n",
            "Target:  <start> robert scores were decent and she can improve if she practices more <end>\n",
            "Predic:  <start> benjamin was was and and and and <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 56 | Train Loss 1.9193 | Val Loss 2.5392\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> benjamin was was and and and and <end> <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 61 | Train Loss 2.0286 | Val Loss 2.6967\n",
            "Source:  alexander male 58 54 52\n",
            "Target:  <start> alexander was average but he was consistent <end>\n",
            "Predic:  <start> james was was and and and and and <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 66 | Train Loss 2.1269 | Val Loss 2.9319\n",
            "Source:  theodore male 59 65 66\n",
            "Target:  <start> theodore scored average marks and there is scope for improvement in her math skills <end>\n",
            "Predic:  <start> james was was was and and and in <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 71 | Train Loss 1.9572 | Val Loss 2.9060\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> mason was was was and and the the in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 76 | Train Loss 1.8646 | Val Loss 3.1130\n",
            "Source:  robert female 58 63 73\n",
            "Target:  <start> robert scores were decent and she can improve if she practices more <end>\n",
            "Predic:  <start> james was was was and and and in <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 81 | Train Loss 2.1167 | Val Loss 2.7147\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> james was was was and and and in in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 86 | Train Loss 1.6281 | Val Loss 2.9609\n",
            "Source:  noah female 69 90 88\n",
            "Target:  <start> noah scored good in reading and writing but her math score was an anomaly <end>\n",
            "Predic:  <start> james was was and and and in in in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 91 | Train Loss 1.9089 | Val Loss 3.1141\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> william was was and and and and in <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 96 | Train Loss 1.9139 | Val Loss 2.9764\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> william was was and and and and and in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 101 | Train Loss 2.1049 | Val Loss 2.8614\n",
            "Source:  adrian male 39 39 34\n",
            "Target:  <start> adrian performed very badly and he failed in writing <end>\n",
            "Predic:  <start> william was was and and and and and <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 106 | Train Loss 1.9215 | Val Loss 2.9800\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> william was was and and and and and <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 111 | Train Loss 1.7543 | Val Loss 3.1365\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> benjamin was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 116 | Train Loss 1.8837 | Val Loss 2.9954\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> benjamin was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 121 | Train Loss 1.6792 | Val Loss 2.9662\n",
            "Source:  christopher female 58 73 68\n",
            "Target:  <start> christopher scored average marks and she can work more on her math <end>\n",
            "Predic:  <start> oliver was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 126 | Train Loss 1.5971 | Val Loss 2.8752\n",
            "Source:  santiago female 47 49 50\n",
            "Target:  <start> santiago performed poorely in the exam and she really needs to put in longer hours <end>\n",
            "Predic:  <start> oliver was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 131 | Train Loss 1.8522 | Val Loss 3.0865\n",
            "Source:  alexander male 58 54 52\n",
            "Target:  <start> alexander was average but he was consistent <end>\n",
            "Predic:  <start> oliver was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 136 | Train Loss 1.7041 | Val Loss 2.8851\n",
            "Source:  andrew female 53 58 65\n",
            "Target:  <start> andrew scored average marks and he can work harder on his math and reading <end>\n",
            "Predic:  <start> oliver was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 141 | Train Loss 1.5750 | Val Loss 2.8819\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> benjamin was was and and and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 146 | Train Loss 1.7874 | Val Loss 3.4070\n",
            "Source:  adrian male 39 39 34\n",
            "Target:  <start> adrian performed very badly and he failed in writing <end>\n",
            "Predic:  <start> benjamin was one of and and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 151 | Train Loss 1.7231 | Val Loss 3.0054\n",
            "Source:  theodore male 59 65 66\n",
            "Target:  <start> theodore scored average marks and there is scope for improvement in her math skills <end>\n",
            "Predic:  <start> benjamin was one of and and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 156 | Train Loss 1.4459 | Val Loss 3.0389\n",
            "Source:  austin female 62 68 68\n",
            "Target:  <start> austin did alright and there is an scope of improvement for her <end>\n",
            "Predic:  <start> william was one of the and the and she in <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 161 | Train Loss 1.4620 | Val Loss 3.0171\n",
            "Source:  adrian male 39 39 34\n",
            "Target:  <start> adrian performed very badly and he failed in writing <end>\n",
            "Predic:  <start> william was one of the and the and she in <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 166 | Train Loss 1.6552 | Val Loss 2.9574\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> william was one of and and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 171 | Train Loss 1.8429 | Val Loss 3.0072\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> william was one of and and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 176 | Train Loss 1.5165 | Val Loss 3.2198\n",
            "Source:  alexander male 58 54 52\n",
            "Target:  <start> alexander was average but he was consistent <end>\n",
            "Predic:  <start> mason was one of the and and and she <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 181 | Train Loss 1.3727 | Val Loss 2.8113\n",
            "Source:  theodore male 59 65 66\n",
            "Target:  <start> theodore scored average marks and there is scope for improvement in her math skills <end>\n",
            "Predic:  <start> william was one of the and the and she in <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 186 | Train Loss 1.3388 | Val Loss 2.7849\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> elijah was one of the and the and she in <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 191 | Train Loss 1.7431 | Val Loss 3.0094\n",
            "Source:  alexander male 58 54 52\n",
            "Target:  <start> alexander was average but he was consistent <end>\n",
            "Predic:  <start> elijah was one of the and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 196 | Train Loss 1.4542 | Val Loss 3.2049\n",
            "Source:  robert female 58 63 73\n",
            "Target:  <start> robert scores were decent and she can improve if she practices more <end>\n",
            "Predic:  <start> elijah was one of the and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 201 | Train Loss 1.5516 | Val Loss 2.5860\n",
            "Source:  santiago female 47 49 50\n",
            "Target:  <start> santiago performed poorely in the exam and she really needs to put in longer hours <end>\n",
            "Predic:  <start> elijah was one of the and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 206 | Train Loss 1.5225 | Val Loss 2.9616\n",
            "Source:  alexander male 58 54 52\n",
            "Target:  <start> alexander was average but he was consistent <end>\n",
            "Predic:  <start> elijah was one of the and and she she well <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 211 | Train Loss 1.5560 | Val Loss 3.1951\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> elijah was one of the and and she in she well\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 216 | Train Loss 1.5866 | Val Loss 2.7812\n",
            "Source:  andrew female 53 58 65\n",
            "Target:  <start> andrew scored average marks and he can work harder on his math and reading <end>\n",
            "Predic:  <start> jacob was one of the and she in she well <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 221 | Train Loss 1.3296 | Val Loss 3.4543\n",
            "Source:  robert female 58 63 73\n",
            "Target:  <start> robert scores were decent and she can improve if she practices more <end>\n",
            "Predic:  <start> jacob was one of the and and she in <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 226 | Train Loss 1.2139 | Val Loss 2.9973\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> jacob was one of the and she in she could well\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 231 | Train Loss 1.3601 | Val Loss 3.1029\n",
            "Source:  christopher female 58 73 68\n",
            "Target:  <start> christopher scored average marks and she can work more on her math <end>\n",
            "Predic:  <start> benjamin was one of the and she in she could well\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 236 | Train Loss 1.5148 | Val Loss 3.3973\n",
            "Source:  robert female 58 63 73\n",
            "Target:  <start> robert scores were decent and she can improve if she practices more <end>\n",
            "Predic:  <start> liam performance was decent and the and she in she could\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 241 | Train Loss 1.2871 | Val Loss 3.0426\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> michael performance was decent and the and she in she could\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 246 | Train Loss 1.2528 | Val Loss 3.2842\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> michael performance was decent and the and she in she could\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 251 | Train Loss 1.3784 | Val Loss 2.8133\n",
            "Source:  ezra male 58 59 58\n",
            "Target:  <start> ezra performance was average though he was consistent <end>\n",
            "Predic:  <start> mason did alright but she can do well <end> <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 256 | Train Loss 1.3298 | Val Loss 2.8998\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> michael performance was decent and the and she more efforts <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 261 | Train Loss 1.1914 | Val Loss 3.0100\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> michael performance was decent and the and she more efforts <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 266 | Train Loss 1.2553 | Val Loss 3.0430\n",
            "Source:  adrian male 39 39 34\n",
            "Target:  <start> adrian performed very badly and he failed in writing <end>\n",
            "Predic:  <start> jacob scored decent marks in the and she will do well\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 271 | Train Loss 1.1088 | Val Loss 3.0579\n",
            "Source:  christopher female 58 73 68\n",
            "Target:  <start> christopher scored average marks and she can work more on her math <end>\n",
            "Predic:  <start> jacob scored decent marks in the and she will do well\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 276 | Train Loss 1.1151 | Val Loss 3.0588\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> jacob scored decent marks in all three the and she will\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 281 | Train Loss 1.0018 | Val Loss 3.3352\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> elijah was one of the and she more efforts <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 286 | Train Loss 0.9772 | Val Loss 3.1264\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> jacob scored decent marks in all three the and she needs\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 291 | Train Loss 0.9945 | Val Loss 3.2953\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> daniel scored decent marks in all three the and she needs\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 296 | Train Loss 0.9047 | Val Loss 3.1504\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> daniel scored decent marks in all three the and she needs\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 301 | Train Loss 1.0320 | Val Loss 3.3678\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> benjamin was consistent and she in the and she could have\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 306 | Train Loss 0.9838 | Val Loss 3.0022\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 311 | Train Loss 0.8537 | Val Loss 3.2264\n",
            "Source:  ezra male 58 59 58\n",
            "Target:  <start> ezra performance was average though he was consistent <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 316 | Train Loss 0.7710 | Val Loss 3.2857\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 321 | Train Loss 0.9794 | Val Loss 3.0726\n",
            "Source:  noah female 69 90 88\n",
            "Target:  <start> noah scored good in reading and writing but her math score was an anomaly <end>\n",
            "Predic:  <start> william was one of the top performers in the class and\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 326 | Train Loss 0.7022 | Val Loss 3.3973\n",
            "Source:  theodore male 59 65 66\n",
            "Target:  <start> theodore scored average marks and there is scope for improvement in her math skills <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 331 | Train Loss 0.7329 | Val Loss 3.4879\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 336 | Train Loss 0.6734 | Val Loss 3.4351\n",
            "Source:  adrian male 39 39 34\n",
            "Target:  <start> adrian performed very badly and he failed in writing <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 341 | Train Loss 0.8411 | Val Loss 3.5543\n",
            "Source:  robert female 58 63 73\n",
            "Target:  <start> robert scores were decent and she can improve if she practices more <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 346 | Train Loss 0.5766 | Val Loss 3.0786\n",
            "Source:  christopher female 58 73 68\n",
            "Target:  <start> christopher scored average marks and she can work more on her math <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she needs <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 351 | Train Loss 0.6473 | Val Loss 3.8677\n",
            "Source:  ethan male 40 52 43\n",
            "Target:  <start> ethan performed really badly in the exams <end>\n",
            "Predic:  <start> daniel scored average marks in all three sections but she needs\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 356 | Train Loss 0.5857 | Val Loss 3.7054\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she could have done\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 361 | Train Loss 0.5263 | Val Loss 3.3919\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she needs <end> <end>\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 366 | Train Loss 0.4618 | Val Loss 3.5766\n",
            "Source:  ryan male 65 54 57\n",
            "Target:  <start> ryan scores were average and he should work harder in reading and writing <end>\n",
            "Predic:  <start> liam performance was consistent and with more efforts she needs to\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 371 | Train Loss 0.6541 | Val Loss 3.9202\n",
            "Source:  hudson male 88 78 75\n",
            "Target:  <start> hudson did very well in the exam and he was among top performers in math <end>\n",
            "Predic:  <start> liam performance was decent and with more efforts she will do\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 376 | Train Loss 0.5297 | Val Loss 3.2581\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> daniel scored average marks in all three sections but she needs\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 381 | Train Loss 0.5049 | Val Loss 3.9534\n",
            "Source:  theodore male 59 65 66\n",
            "Target:  <start> theodore scored average marks and there is scope for improvement in her math skills <end>\n",
            "Predic:  <start> daniel scored average marks in all three sections but she could\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 386 | Train Loss 0.4396 | Val Loss 3.1773\n",
            "Source:  theodore male 59 65 66\n",
            "Target:  <start> theodore scored average marks and there is scope for improvement in her math skills <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she needs to work\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 391 | Train Loss 0.7071 | Val Loss 3.7396\n",
            "Source:  jayden female 62 70 75\n",
            "Target:  <start> jayden s performance was decent but she needs to work on her math <end>\n",
            "Predic:  <start> oliver was consistent and with more efforts she will do well\n",
            "----------------------<><><><><>-----------------------\n",
            "Epoch 396 | Train Loss 0.3502 | Val Loss 3.5337\n",
            "Source:  lucas male 40 43 39\n",
            "Target:  <start> lucas nearly failed the exams his performance was very poor <end>\n",
            "Predic:  <start> daniel scored average marks in all three sections but she needs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsqEm6Y-Ihqq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}