{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentive Seq2Seq with copying mechanism for Table-to-Text \n",
    "\n",
    "In this ipynb notbook, we'll be building a deep learning based Sequence-to-Sequence (Seq2Seq) model in an attempt to generate textual sequence on a tabular data, using PyTorch and TorchText. This will be done on a small fabricated dataset (student_grade_comments), but the models can be applied to any dataset that has tabular subject (attribute), values (cell) and annotation (caption) column. This notebook will not talk about every concept and reasoning behind the network representation and structure, that work has been left for the final word document which would be in form of an exhaustive report. Let's start with the basics first.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The most common sequence-to-sequence (seq2seq) models are *encoder-decoder* models, which commonly use a *recurrent neural network* (RNN) to *encode* the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a *context vector*. We can think of the context vector as being an abstract representation of the entire input sentence. This vector is then *decoded* by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n",
    "\n",
    "![](/assets/seq2seq1.png)\n",
    "\n",
    "The above image shows an example translation. The input/source sentence, \"guten morgen\", is passed through the embedding layer (yellow) and then input into the encoder (green). We also append a *start of sequence* (`<sos>` or `<START>`, whichever one would like to trigger the decoder with) and *end of sequence* (`<eos>` or `<END>`) token to the start and end of sentence, respectively. At each time-step, the input to the encoder RNN is both the embedding, $e$, of the current word, $e(x_t)$, as well as the hidden state from the previous time-step, $h_{t-1}$, and the encoder RNN outputs a new hidden state $h_t$. We can think of the hidden state as a vector representation of the sentence so far. The RNN can be represented as a function of both of $e(x_t)$ and $h_{t-1}$:\n",
    "\n",
    "Table-to-text can be represented in similar manner, where a combination (... more on this in the report) of table attributes and corresponding cell values, is passed as a input sequence to the encoder then the content description is produced by the decoder. To represent the encoder-decoder model in mathematical form...\n",
    "\n",
    "$$h_t = \\text{EncoderRNN}(e(x_t), h_{t-1})$$\n",
    "\n",
    "We're using the term RNN generally here, it could be any recurrent architecture, such as an *LSTM* (Long Short-Term Memory) or a *GRU* (Gated Recurrent Unit). In our case, we have gone ahead with the *GRU* due to it's simpler architecture. \n",
    "\n",
    "Here, we have $X = \\{x_1, x_2, ..., x_T\\}$, where $x_1 = \\text{<sos>}, x_2 = \\text{guten}$, etc. The initial hidden state, $h_0$, is usually either initialized to zeros or a learned parameter.\n",
    "\n",
    "Once the final word, $x_T$, has been passed into the RNN via the embedding layer, we use the final hidden state, $h_T$, as the context vector, i.e. $h_T = z$. This is a vector representation of the entire source sentence.\n",
    "\n",
    "Now we have our context vector, $z$, we can start decoding it to get the output/target sentence, \"good morning\". Again, we append start and end of sequence tokens to the target sentence. At each time-step, the input to the decoder RNN (blue) is the embedding, $d$, of current word, $d(y_t)$, as well as the hidden state from the previous time-step, $s_{t-1}$, where the initial decoder hidden state, $s_0$, is the context vector, $s_0 = z = h_T$, i.e. the initial decoder hidden state is the final encoder hidden state. Thus, similar to the encoder, we can represent the decoder as:\n",
    "\n",
    "$$s_t = \\text{DecoderRNN}(d(y_t), s_{t-1})$$\n",
    "\n",
    "Although the input/source embedding layer, $e$, and the output/target embedding layer, $d$, are both shown in yellow in the diagram they are two different embedding layers with their own parameters.\n",
    "\n",
    "In the decoder, we need to go from the hidden state to an actual word, therefore at each time-step we use $s_t$ to predict (by passing it through a `Linear` layer, shown in purple) what we think is the next word in the sequence, $\\hat{y}_t$. \n",
    "\n",
    "$$\\hat{y}_t = f(s_t)$$\n",
    "\n",
    "The words in the decoder are always generated one after another, with one per time-step. We always use `<sos>` for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\\hat{y}_{t-1}$. This is called *teacher forcing*, see a bit more info about it [here](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/). \n",
    "\n",
    "When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference it is common to keep generating words until the model outputs an `<eos>` token or after a certain amount of words have been generated.\n",
    "\n",
    "Once we have our predicted target sentence, $\\hat{Y} = \\{ \\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T \\}$, we compare it against our actual target sentence, $Y = \\{ y_1, y_2, ..., y_T \\}$, to calculate our loss. We then use this loss to update all of the parameters in our model.\n",
    "\n",
    "## Preparing Data\n",
    "\n",
    "We'll be coding up the models in PyTorch and using TorchText to help us do all of the pre-processing required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYoIaaQHTCaT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Import Relevant Libraries: \n",
    "    For this exercise, we will be needing below mentioned libraries. \n",
    "    In case module is found absent, look upto pip install or conda install commands for installations.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "#from gensim.models import FastText\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbowAMISDjHJ"
   },
   "outputs": [],
   "source": [
    "'''Import the .csv file containing table rows and corresponding annotations'''\n",
    "\n",
    "df = pd.read_csv('student_grade_comments.csv')\n",
    "source_df = df.drop(['comments'], axis=1)\n",
    "source_col = list(source_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>math</th>\n",
       "      <th>reading</th>\n",
       "      <th>writing</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>liam</td>\n",
       "      <td>female</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "      <td>liam performance was decent and she was consis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>noah</td>\n",
       "      <td>female</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>noah scored good in reading and writing but he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>william</td>\n",
       "      <td>female</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "      <td>william was one of the top performers in the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>james</td>\n",
       "      <td>male</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "      <td>james performed poorly across all three subjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>oliver</td>\n",
       "      <td>male</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>oliver was consistent and with more efforts he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  gender  math  reading  writing  \\\n",
       "0     liam  female    72       72       74   \n",
       "1     noah  female    69       90       88   \n",
       "2  william  female    90       95       93   \n",
       "3    james    male    47       57       44   \n",
       "4   oliver    male    76       78       75   \n",
       "\n",
       "                                            comments  \n",
       "0  liam performance was decent and she was consis...  \n",
       "1  noah scored good in reading and writing but he...  \n",
       "2  william was one of the top performers in the c...  \n",
       "3  james performed poorly across all three subjec...  \n",
       "4  oliver was consistent and with more efforts he...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4W0PFBiXUwU"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "converting the tabular attributes and cells into sequential form to feed into encoder and decoder:\n",
    "eg: Table\n",
    "    name | gender | math | reading | writing  [attributes]\n",
    "    noah | female | 69   | 90      | 88       [cell values]\n",
    "sequence:\n",
    "    'name noah gender female math 69 reading 90 writing 88' \n",
    "    (this representation is important, the argument behind it is presented in the report)\n",
    "'''\n",
    "\n",
    "source = list()\n",
    "for index, row in source_df.iterrows():\n",
    "    source_seq = list()\n",
    "    for col in source_col:\n",
    "        source_seq.append(col)\n",
    "        source_seq.append(str(row[col]))\n",
    "    source.append(' '.join(source_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIKWYl-yXUww"
   },
   "outputs": [],
   "source": [
    "'''Putting the source/input sequence and target/output annotations in two different columns of a dataframe'''\n",
    "\n",
    "data = pd.DataFrame({'Text': source, 'Summary': df['comments']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7lqLjyLLDXa"
   },
   "outputs": [],
   "source": [
    "x = data['Text']  # input/source sequence\n",
    "y = data['Summary'] #output/target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BlfwPHtNXI9"
   },
   "outputs": [],
   "source": [
    "''' define a function to clean the text as suited using regex'''\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    '''\n",
    "    perform other regex operations as per model requirement\n",
    "    '''\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "969v-EnfDlun",
    "outputId": "32e6c313-611b-447a-970a-70039516e066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name alexander gender male math 58 reading 54 writing 52\n",
      "<START> alexander was average but he was consistent. <END>\n"
     ]
    }
   ],
   "source": [
    "'''clean the sequences and put them in the list'''\n",
    "\n",
    "cleaned_source = list(map(clean,x))\n",
    "cleaned_summary = list(map(clean,y))\n",
    "\n",
    "'''now adding the <START> and <END> tokens at the extermes of target sequences'''\n",
    "for i in range(len(cleaned_summary)):\n",
    "    cleaned_summary[i] = \"<START> \" + cleaned_summary[i] + \" <END>\"\n",
    "\n",
    "print(cleaned_source[10])\n",
    "print(cleaned_summary[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RUaNE15DpNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source length is:  10\n",
      "Maximum target length is:  23\n"
     ]
    }
   ],
   "source": [
    "'''check out the maximum source and target length, to be used in the model later'''\n",
    "\n",
    "max_source_length = max([len(text.split()) for text in cleaned_source])\n",
    "max_summary_length = max([len(text.split()) for text in cleaned_summary])\n",
    "\n",
    "print('Maximum source length is: ', max_source_length)\n",
    "print('Maximum target length is: ', max_summary_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SDHHGErkXUyd"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "use sklearn train_test_split method to split the data into training and validation (for validation).\n",
    "one can further split the dataset to allocate a minor portion of it for testing\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_source, test_source, new_summary, test_summary = train_test_split(cleaned_source, cleaned_summary, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabulary\n",
    "\n",
    "In this section, we build the soruce and target vocabulary from the training data. Vocabulary is presented in the form dictionary object mapping words to their corresponding indices and vice versa. \n",
    "\n",
    "A problem peculiar with Table-to-text task is the reprsentation of low frequency words like 'named entity', if we try to enlarge the decoder vocabulary and build it on full training summary, there would be still be instances during validation or testing where model would encounter a new word and unable to find that in the vocabulary and that too at expense of large look up table leading to slow training.\n",
    "\n",
    "To deal with the problem mentioned above, we train the decoder on small vocabulary comprising only most frequent words and try to copy the unseen words represented in the encoder vocabulary through *Copying Mechanism* which we will talk about later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otf9wqD83OxL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Vocab objects:\n",
    "    word2Index_enc: dictionary containing all words in source corpus and their index\n",
    "    \n",
    "    word2Index_dec_big: dictionary containing all words in target corpus and their index\n",
    "    word2Index_dec: dictionary containing most frquent words in target corpus \n",
    "                    and their original index word2Index_dec_big\n",
    "    word2PsuInd_dec: pseudo dictionary containing most frquent words in target corpus but with new serial index\n",
    "    \n",
    "    len(word2PsuInd_dec) == len(word2Index_dec) (length of vocab on which decoder is trained)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from collections import OrderedDict \n",
    "\n",
    "decoder_word_freq = 2\n",
    "\n",
    "word2Index_enc = {}   \n",
    "word2Index_dec = {}   \n",
    "word2Index_dec_big = {} \n",
    "\n",
    "ind2Word_enc = {}\n",
    "ind2Word_dec = {}\n",
    "ind2Word_dec_big = {}\n",
    "\n",
    "word2PsuInd_dec = {}  \n",
    "psuInd2Word_dec = {}\n",
    "\n",
    "encoder_paragraph = list(set((' '.join(new_source)).split()))\n",
    "\n",
    "decoder_paragraph_list = list((' '.join(new_summary)).split())\n",
    "decoder_dict = OrderedDict()\n",
    "for word in decoder_paragraph_list:\n",
    "    try:\n",
    "        decoder_dict[word] = decoder_dict[word] + 1\n",
    "    except:\n",
    "        decoder_dict[word] = 1\n",
    "\n",
    "ind2Word_enc[0] = '<UNK>'\n",
    "ind2Word_dec[0] = '<UNK>'\n",
    "word2Index_enc['<UNK>'] = 0\n",
    "word2Index_dec['<UNK>'] = 0\n",
    "ind2Word_dec_big[0] = '<UNK>'\n",
    "word2Index_dec_big['<UNK>'] = 0\n",
    "word2PsuInd_dec['<UNK>'] = 0\n",
    "psuInd2Word_dec[0] = '<UNK>'\n",
    "\n",
    "dec_index = 1\n",
    "for (decoder_dict_word, decoder_dict_number) in decoder_dict.items():\n",
    "    word2Index_dec_big[decoder_dict_word] = dec_index  \n",
    "    ind2Word_dec_big[dec_index] = decoder_dict_word\n",
    "    if decoder_dict_number >= 2 :                      \n",
    "        word2Index_dec[decoder_dict_word] = dec_index     \n",
    "        ind2Word_dec[dec_index] = decoder_dict_word\n",
    "        psuedo_index = len(word2PsuInd_dec.keys())\n",
    "        word2PsuInd_dec[decoder_dict_word] = psuedo_index \n",
    "        psuInd2Word_dec[psuedo_index] = decoder_dict_word\n",
    "    dec_index+=1\n",
    "\n",
    "enc_index = 1\n",
    "for index,word in enumerate(encoder_paragraph):\n",
    "    if word != ' ':\n",
    "        word2Index_enc[word] = enc_index    \n",
    "        ind2Word_enc[enc_index] = word \n",
    "        enc_index+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "First, we'll build the encoder. We only use a single layer GRU, we also have a flexibility to use a *bidirectional RNN*. With a bidirectional RNN, we have two RNNs in each layer. A *forward RNN* going over the embedded sentence from left to right (shown below in green), and a *backward RNN* going over the embedded sentence from right to left (teal). All we need to do in code is set `bidirectional = True` and then pass the embedded sentence to the RNN as before. But for simplicity we will not choose the bidirectionality option unless required.\n",
    "\n",
    "![](/assets/seq2seq8.png)\n",
    "\n",
    "Mathematically a full bidirection GRU can be represented by:\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t^\\rightarrow &= \\text{EncoderGRU}^\\rightarrow(e(x_t^\\rightarrow),h_{t-1}^\\rightarrow)\\\\\n",
    "h_t^\\leftarrow &= \\text{EncoderGRU}^\\leftarrow(e(x_t^\\leftarrow),h_{t-1}^\\leftarrow)\n",
    "\\end{align*}$$\n",
    "\n",
    "Where $x_0^\\rightarrow = \\text{<sos>}, x_1^\\rightarrow = \\text{guten}$ and $x_0^\\leftarrow = \\text{<eos>}, x_1^\\leftarrow = \\text{morgen}$.\n",
    "\n",
    "We can pass an input (`embedded_outputs' and 'prev_hidden_state`) to the RNN, which tells PyTorch to initialize both the forward and backward initial hidden states ($h_0^\\rightarrow$ and $h_0^\\leftarrow$, respectively) to a tensor of all zeros. We'll also get two context vectors, one from the forward RNN after it has seen the final word in the sentence, $z^\\rightarrow=h_T^\\rightarrow$, and one from the backward RNN after it has seen the first word in the sentence, $z^\\leftarrow=h_T^\\leftarrow$.\n",
    "\n",
    "The RNN returns `output` and `prev_hidden_state`. \n",
    "\n",
    "`output` is of size **[src len, batch size, hid dim * num directions]** where the first `hid_dim` elements in the third axis are the hidden states from the top layer forward RNN, and the last `hid_dim` elements are hidden states from the top layer backward RNN. We can think of the third axis as being the forward and backward hidden states concatenated together other, i.e. $h_1 = [h_1^\\rightarrow; h_{T}^\\leftarrow]$, $h_2 = [h_2^\\rightarrow; h_{T-1}^\\leftarrow]$ and we can denote all encoder hidden states (forward and backwards concatenated together) as $H=\\{ h_1, h_2, ..., h_T\\}$.\n",
    "\n",
    "`prev_hidden_state` is of size **[n layers * num directions, batch size, hid dim]**, where **[-2, :, :]** gives the top layer forward RNN hidden state after the final time-step (i.e. after it has seen the last word in the sentence) and **[-1, :, :]** gives the top layer backward RNN hidden state after the final time-step (i.e. after it has seen the first word in the sentence).\n",
    "\n",
    "As the decoder is not bidirectional, it only needs a single context vector, $z$, to use as its initial hidden state, $s_0$, and in case of birectional decoder, we will have two, a forward and a backward one ($z^\\rightarrow=h_T^\\rightarrow$ and $z^\\leftarrow=h_T^\\leftarrow$, respectively). We solve this by concatenating the two context vectors together, passing them through a linear layer, $g$, and applying the $\\tanh$ activation function. \n",
    "\n",
    "$$z=\\tanh(g(h_T^\\rightarrow, h_T^\\leftarrow)) = \\tanh(g(z^\\rightarrow, z^\\leftarrow)) = s_0$$\n",
    "\n",
    "As we want our model to look back over the whole of the source sentence we return `output`, the stacked forward and backward hidden states for every token in the source sentence. We also return `prev_hidden_state`, which acts as our initial hidden state in the decoder.\n",
    "\n",
    "In case of unidirectional forward encoder, things are going to be much simpler. Every mathematical representation holds true as above, we just ignore the backward nature of encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERJFNd_0XU1u"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Args:\n",
    "        input_vocab_size: (int) Size of source vocabulary\n",
    "        embed_size: (int) Embedding dimensions\n",
    "        hidden_size: (int) Dimensions of hidden state\n",
    "        num_layers: (int) Number of stacked GRU layers, default is 1\n",
    "        bidirectional: (Bool) If RNN is required to be birectional in nature, default is False\n",
    "    '''\n",
    "  \n",
    "    def __init__(self,input_vocab_size, embed_size, hidden_size,num_layers=1,bidirectional=False):\n",
    "        super(Encoder,self).__init__()\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
    "\n",
    "        self.gru_layer = nn.GRU(embed_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self,input_,prev_hidden_state):\n",
    "        '''Arg:\n",
    "            input_: Tensor of source word indices [source length x batch size]\n",
    "                    (in this case 'batch size' = 1)\n",
    "            prev_hidden_state: Previous hidden state [n_layers*n direction x batch size x hidden dim]\n",
    "                                (in this case n_layers, n direction, batch size = 1)\n",
    "        '''\n",
    "        input_tensor = input_.view(-1,1)\n",
    "        #input_tensor = [source length x batch size]\n",
    "        embedded_outputs = self.embedding(input_tensor).view(1,1,-1)\n",
    "        #embedded_outputs = [source length x batch size x embed dim]\n",
    "\n",
    "        output, prev_hidden_state = self.gru_layer(embedded_outputs,prev_hidden_state)\n",
    "        #prev_hidden_state = [n_layers*n direction x batch size x hidden dim]\n",
    "        #output = [source length x batch size x hidden dim*n direction]\n",
    "\n",
    "        return output, prev_hidden_state\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionDecoder\n",
    "\n",
    "**Attention**\n",
    "\n",
    "The attention layer mechanism will take in the previous hidden state of the decoder, $s_{t-1}$, and hidden states (all of the stacked forward and backward in case of bidirectional encoder) from the encoder, $H$. The layer will output an attention vector, $a_t$, that is the length of the source sentence, each element is between 0 and 1 and the entire vector sums to 1. We ensure the attention vector fits the constraints of having all elements between 0 and 1 and the vector summing to 1 by passing it through a $\\text{softmax}$ layer.\n",
    "\n",
    "Intuitively, this layer takes what we have decoded so far, $s_{t-1}$, and all of what we have encoded, $H$, to produce a vector, $a_t$, that represents which words in the source sentence we should pay the most attention to in order to correctly predict the next word to decode, $\\hat{y}_{t+1}$. \n",
    "\n",
    "This gives us the attention over the source sentence!\n",
    "\n",
    "Graphically, this looks something like below. This is for calculating the very first attention vector, where $s_{t-1} = s_0 = z$. The green/teal blocks represent the hidden states from both the forward and backward RNNs, and the attention computation is all done within the pink block.\n",
    "\n",
    "![](/assets/seq2seq9.png)\n",
    "\n",
    "**Decoder**\n",
    "\n",
    "Next, the attention distribution is used to produce a weighted sum of the encoder hidden states, known as the context vector $w_t$, which is given by `attention_applied`:\n",
    "\n",
    "$$w_t = \\Sigma a_i^th_i$$\n",
    "\n",
    "The context vector, which can be seen as a fixed size representation of what has been read from the source for this step, is concatenated with the decoder state st and fed through two linear layers to produce the vocabulary distribution $P_{vocab}$\n",
    "\n",
    "$$P_{vocab} = \\text{softmax}(V^* (V[s_t; w_t] + b) + b^*)$$\n",
    "\n",
    "where $V$, $V^*$, $b$ and $b^*$ are learnable parameters. $P_{vocab}$ is a probability distribution over all words in the vocabulary, and provides us with our final distribution from which to predict words $w$:\n",
    "\n",
    "$$P(w) = P_{vocab}(w)$$\n",
    "\n",
    "The embedded (with dropout) input word, $d(y_t)$, the weighted source vector, $w_t$ is then concatenated to form `attention_combine_relu`, which in turn is passed with the previous decoder hidden state 'prev_hidden_state', $s_{t-1}$, into the decoder RNN to produce `output` and `hidden`.\n",
    "\n",
    "$$s_t = \\text{DecoderGRU}(d(y_t), w_t, s_{t-1})$$\n",
    "\n",
    "We then pass `output` through the linear layer, $Linear$ and apply $logsoftmax$ to make a prediction of the next word in the target sentence, $\\hat{y}_{t+1}$.\n",
    "\n",
    "$$\\hat{y}_{t+1} = LogSoftmax(Linear(output))$$\n",
    "\n",
    "The image below shows decoding the first word in an example translation.\n",
    "\n",
    "![](/assets/seq2seq10.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I29Rw8CA7esY"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "    '''\n",
    "    Args:\n",
    "        output_vocab_size: (int) Size of target vocabulary\n",
    "        embed_dim: (int) Embedding dimensions\n",
    "        hidden_size: (int) Dimensions of hidden state\n",
    "        max_length_encoder: (int) Maximum length of encoder sequence\n",
    "        dropout_value: (float) Value between 0 & 1\n",
    "        num_layers: (int) Number of stacked GRU layers, default is 1\n",
    "    ''' \n",
    "\n",
    "    def __init__(self, output_vocab_size, embed_dim, hidden_size, max_length_encoder, dropout_value, num_layers=1):\n",
    "        super(AttentionDecoder,self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.dropout_p = dropout_value\n",
    "        self.max_length_encoder = max_length_encoder\n",
    "\n",
    "        self.embedding = nn.Embedding(output_vocab_size, embed_dim) \n",
    "\n",
    "        self.attention_layer = nn.Linear(hidden_size*2, max_length_encoder)\n",
    "        self.attention_combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "        self.s_layer = nn.Linear(hidden_size, 1)\n",
    "        self.x_layer = nn.Linear(hidden_size, 1)\n",
    "        self.context_layer = nn.Linear(hidden_size, 1)\n",
    "        self.linear_pgen = nn.Linear(3, 1)\n",
    "\n",
    "        self.gru_layer = nn.GRU(embed_dim, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(self.dropout_p)    \n",
    "\n",
    "    def forward(self,input_tens,prev_hidden_state,encoder_output):\n",
    "        '''\n",
    "        Args:\n",
    "            input_tens = [1 x batch size] (seq length is strictly 1, batch size in our case 1)\n",
    "            prev_hidden_state = Final encoder state [n_layers x batch size x hidden dim]\n",
    "                                (in our case, n_layers and batch size is 1)\n",
    "            encoder_output = encoder final output over source sequence [max_length_encoder x hidden dim]\n",
    "        '''\n",
    "        embedded_outputs = self.embedding(input_tens).view(1,1,-1)\n",
    "        #input_tens = [1 x batch size]\n",
    "        #embedded_outputs = [1 x batch size x embed dim]\n",
    "\n",
    "        embeddings_dropout = self.dropout_layer(embedded_outputs)\n",
    "        #embeddings_dropout = [1 x batch size x embed dim]\n",
    "        \n",
    "        '''\n",
    "        Attention Portion:\n",
    "        '''\n",
    "        #prev_hidden_state = [n_layers x batch size x hidden dim]\n",
    "        attention_layer_output = self.attention_layer(torch.cat((embeddings_dropout[0],prev_hidden_state[0]),1)) \n",
    "        \n",
    "        #cat = [batch size x (embed dim + hidden dim)] = [batch size x 2*(hidden dim)]\n",
    "        #in our case emdedding dimension is going to be same as hidden dimension\n",
    "        #attention_layer_output = [batch size x max_length_encoder]\n",
    "\n",
    "        attention_weights = nn.functional.softmax(attention_layer_output,dim=1)\n",
    "        #attention_weights = [batch size x max_length_encoder]\n",
    "        \n",
    "        '''\n",
    "        Decoder Portion:\n",
    "        '''\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0),encoder_output.unsqueeze(0))\n",
    "        #attention_weights = [batch size x max_length_encoder], after unsqueezing in 0th dim ==> [1 x batch size x max_length_encoder]\n",
    "        #encoder_output = [max_length_encoder x hidden dim], after unsqueezing in 0th dim ==> [1 x max_length_encoder x hidden dim] \n",
    "        #attention_applied = [1 x batch size x hidden dim]\n",
    "\n",
    "        attention_combine_logits = self.attention_combine(torch.cat((embeddings_dropout[0],attention_applied[0]),1)).unsqueeze(0)  #since gru requires a batch dimension\n",
    "        #embeddings_dropout = [1 x batch size x embed dim]\n",
    "        #attention_applied = [1 x batch size x hidden dim]\n",
    "        #cat = [batch size x (embed dim + hidden dim)] = [batch size x 2*(hidden dim)]\n",
    "        #attention_combine_logits = [batch size x hidden dim], after unsqueezing in 0th dim ==> [1 x batch size x hidden dim]\n",
    "\n",
    "        attention_combine_relu = nn.functional.relu(attention_combine_logits)\n",
    "        #attention_combine_relu = [1 x batch size x hidden dim]\n",
    "        \n",
    "        '''\n",
    "        Pgen calculation used for copying\n",
    "        '''\n",
    "        s_output = self.s_layer(prev_hidden_state[0])\n",
    "        #prev_hidden_state = [n_layers x batch size x hidden dime]\n",
    "        #s_output = [batch size x 1] = [1 x 1]\n",
    "\n",
    "        x_output = self.x_layer(embeddings_dropout[0])\n",
    "        #embeddings_dropout = [1 x batch size x embed dim]\n",
    "        #x_output = [batch size x 1] = [1 x 1] as (hidden dim = embed dim)\n",
    "\n",
    "        context = torch.flatten(attention_applied)\n",
    "        #attention_applied = [1 x batch size x hidden dim]\n",
    "        #context = [batch size * hidden dim] = [hidden dim]\n",
    "\n",
    "        context_weights = self.context_layer(attention_applied)\n",
    "        #context_weights = [1 x batch size x 1] = [1 x 1 x 1]\n",
    "\n",
    "        sx = torch.cat((s_output[0],x_output[0]),0)\n",
    "        #sx = [1 x 2*(unit)]\n",
    "        sxc = torch.cat((sx,context_weights[0][0]),0)\n",
    "        #sxc = [1 x 3*(unit)]\n",
    "        linear_pgen = self.linear_pgen(sxc)\n",
    "        #linear_pgen = [1 x 1]\n",
    "        m = nn.Sigmoid()\n",
    "        pgen = m(linear_pgen)\n",
    "        #pgen = [1 x 1]\n",
    "\n",
    "        output,hidden = self.gru_layer(attention_combine_relu,prev_hidden_state)\n",
    "        #attention_combine_relu = [1 x batch size x hidden dim]\n",
    "        #prev_hidden_state = [n_layers x batch size x hidden dime]\n",
    "        #output = [1 x batch size x hidden dim] =[1 x 1 x hidden dim]\n",
    "        #hidden = [n_layers x batch size x hidden dime] = [1 x 1 x hidden dim]\n",
    "\n",
    "        output_logits = self.output_layer(output)\n",
    "        #output_logits = [1 x batch size x output vocab size]\n",
    "        output_softmax = nn.functional.log_softmax(output_logits[0],dim=1)\n",
    "        #output_softmax = [batch size x output vocab size] = [1 x output vocab size] \n",
    "        #softmax applied distribution over target vocab\n",
    "        return output_softmax,hidden,attention_weights,pgen\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Seq2Seq with Attention\n",
    "\n",
    "The model may attend to relevant words in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary Germany beat Argentina 2-0 the model may attend to the words victorious and win in the source text.\n",
    "\n",
    "![](/assets/basic_seq2seq_attention.png)\n",
    "\n",
    "Descriptions provided in the Encoder and AttentionDecoder section would be enough for modelling this type of netwok. But this kind of vanila seq2seq model has obvious shortcomings perticularly when exhaustive vocabulary has more representations of low frequency words. Keeping the full vocab will result in larger training time but removal of low frequent words leads to substantial loss of information. This situation is more likely to arise in table to text generation tasks where tabular contents are very specific and include large number of name entities. This problem can be overcome using copying or pointing mechanism.\n",
    "\n",
    "## Seq2Seq with Attention and Copying Mechanism\n",
    "\n",
    "For each decoder timestep a generation probability $P_{gen}$ $\\epsilon$ $[0,1]$ is calculated, which weights the probability of generating words from the vocabulary, versus copying words from the source text. The vocabulary distribution and the attention distribution are weighted and summed to obtain the final distribution, from which we make our prediction. Note that out-of-vocabulary article words such as 2-0 are included in the final distribution.\n",
    "\n",
    "![](/assets/seq2seq_with_pg.png)\n",
    "\n",
    "In the pointer-generator model (depicted in Figure abthe attention distribution at and context vector $w_t$ are calculated as described in AttentionDecoder section. In addition, the generation probability $P_{gen}$ $\\epsilon$ [0;1] for timestep t is calculated from the context vector $w_t$, the decoder state $s_t$ and the decoder input $x_t$ as following:\n",
    "\n",
    "$$P_{gen} = Sigmoid(W_h^Tw_t+W_s^Ts_t+W_x^Tx_t+b_{pg})$$\n",
    "\n",
    "Next, $P_{gen}$ is used as a soft switch to choose between generating a word from the vocabulary by\n",
    "sampling from $P_{vocab}$, or copying a word from the input sequence by sampling from the attention distribution $a_t$ . For each source sequence, the extended vocabulary `extended_vocab` represents the union of the decoder vocabulary, and all words appearing in the source sequence which are absent in decoder vocab. We obtain the following probability distribution over the extended vocabulary or `P_over_extended_vocab` as:\n",
    "\n",
    "$$P(w) = P_{gen}P_{vocab}(w)+(1-P_{gen})\\Sigma_{i:w_i=w}a_i^t$$\n",
    "\n",
    "If $w$ is an out-of-vocabulary (OOV) word, then $P_{vocab}(w)$ is zero; similarly if $w$ does not appear in the source document, then $\\Sigma_{i:w_i=w}a_i^t$ at $i$ is zero. The ability to produce OOV words is one of the primary advantages of pointing or copying mehanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of encoder vocab:  155\n",
      "Size of decoder vocab: Full 236 | Frequent 110\n"
     ]
    }
   ],
   "source": [
    "'''let us check the vocab size once again'''\n",
    "\n",
    "print('Size of encoder vocab: ',len(word2Index_enc))\n",
    "print('Size of decoder vocab: Full {} | Frequent {}'.format(len(word2Index_dec_big),len(word2Index_dec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Loss\n",
    "\n",
    "Our next step would be to define the training and validation loss and this is done in `train` and `validate` function. Train method is defined below, we allow the model to randomly choose between 'Teacher forcing' or 'No teacher forcing' by imposing a threshold `teacher_forcing_ratio`. \n",
    "\n",
    "    - if decoder is teacher forced, actual reference token (ground truth) is used as next decoder input\n",
    "    - if not teacher forced, decoder output produced in the previous step is used as next decoder input\n",
    "\n",
    "During training, the loss for timestep $t$ is the negative log likelihood of the target word $w_t^*$ for that\n",
    "timestep, i.e.:\n",
    "\n",
    "$$loss_t = -log(P(w_t^*))$$\n",
    "\n",
    "and the overall loss for the whole sequence is:\n",
    "\n",
    "$$loss = \\Sigma_{t=0}^Tloss_t$$\n",
    "\n",
    "validate step is similar to train step but in the absence of Teacher forcing and varying gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y40789vQM8gc"
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, input_tensor, target_tensor, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length, iters, \n",
    "          teacher_forcing_ratio = 0.4, clip = 0.4):\n",
    "    '''\n",
    "    Arg:\n",
    "        encoder: encoder model to train\n",
    "        decoder: decoder model to train\n",
    "        input_tensor: source seq in tensor form [seq length x 1] batch size = 1\n",
    "        target_tensor: target seq in tensor form [seq length x 1] batch size = 1\n",
    "        encoder_optimizer: optimizer for encoder\n",
    "        decoder_optimizer: optimizer for decoder\n",
    "        citerion: Loss criterion\n",
    "        max length: maximum source length \n",
    "        iters: number of iterations\n",
    "        teacher_forcing_ratio: if teacher forcing, actual next token is useed as next input\n",
    "        clip: to prevent gradients from exploding \n",
    "    '''\n",
    "    encoder_optimizer.zero_grad() #initialize encoder_optimizer at zero gradient\n",
    "    decoder_optimizer.zero_grad() #initialize decoder_optimizer at zero gradient\n",
    "\n",
    "    #prev_unk_word = ''\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    #encoder_hidden = [1 x 1 x hidden dim]\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = target_tensor.size(0)\n",
    "\n",
    "    for encoder_index in range(0, input_length):\n",
    "        encoder_output,encoder_hidden = encoder(input_tensor[encoder_index], encoder_hidden)\n",
    "        #input_tensor[encoder_index] = [1 x 1 x embed dim] (embed dim = hidden dim)\n",
    "        #encoder_hidden = [1 x 1 x hidden dim] {encoder arg inp}\n",
    "        #encoder_hidden = [n_layers*n direction x 1 x hidden dime] {encoder product}\n",
    "        #encoder_output = [seq length x 1 x hidden dim*n direction]\n",
    "        #seq length, n_layers, n direction = 1  \n",
    "\n",
    "        encoder_outputs[encoder_index] = encoder_output[0,0] # [1 x hidden dim]\n",
    "        #encoder_outputs: [seq length x hidden dim] ==> [seq length x hidden dim] (hidden state from all 0 to new)\n",
    "\n",
    "    decoder_input = torch.tensor([word2Index_dec['<START>']],device=device)\n",
    "    #decoder_input = [1 x 1]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #decoder_hidden = [1 x 1 x hidden dim]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    extended_vocab = psuInd2Word_dec.copy()\n",
    "    reverse_extended_vocab = word2PsuInd_dec.copy()\n",
    "    duplicate_words = {}\n",
    "    extend_key = len(word2Index_dec.keys())\n",
    "    input_list = input_tensor.tolist()\n",
    "    i =0\n",
    "    for input_word in input_list:\n",
    "        if ind2Word_enc[input_word[0]] in word2Index_dec.keys():\n",
    "            duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word[0]]]\n",
    "        else:\n",
    "            extended_vocab[extend_key] = ind2Word_enc[input_word[0]]\n",
    "            reverse_extended_vocab[ind2Word_enc[input_word[0]]] = extend_key\n",
    "            extend_key += 1\n",
    "        i = i+1\n",
    "    '''\n",
    "    Note:\n",
    "        - Words appearing both in Input/Source sequence is copied in 'duplicate_words' dictionary.\n",
    "        - New unseen words for decoder i.e. Target OOV is copied as an extension of Pseudo decoder \n",
    "          vocabulary i.e. 'extended_vocab'\n",
    "        - Both of these vocabularies play vital role during copying \n",
    "    '''\n",
    "    \n",
    "    loss = 0\n",
    "    for decoder_index in range(output_length):\n",
    "        decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "        #decoder_input = [1 x 1]\n",
    "        #decoder_hidden = [1 x 1 x hidden dim]\n",
    "        #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "        #decoder_output = [1 x output vocab size]\n",
    "        #decoder_hidden = [1 x 1 x hidden dime]\n",
    "        #decoder_attention = [1 x max source length]\n",
    "        #pgen = [1 x 1]\n",
    "\n",
    "        P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "        #P_over_extended_vocab = [1 x output vocab size] (exp(decoder_output)*pgen)\n",
    "\n",
    "        decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "        #restricting decoder attention upto only input length\n",
    "        #decoder_attention = [1 x input_length]\n",
    "        p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "        #p_duplicate_list = [input_length x output vocab size] \n",
    "\n",
    "        p_duplicate_list = p_duplicate_list.tolist()\n",
    "        for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "            p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1 #making duplicate key,vals apparent\n",
    "      \n",
    "        p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "        p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "        #p_diag = [1 x output vocab size]\n",
    "\n",
    "        p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "        #p_diag = p_diag*(1 - pgen)\n",
    "\n",
    "        p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0) #p_diag.squeeze(0) ==> [output vocab size]\n",
    "        #p_add_diag = [output vocab size x output vocab size]\n",
    "\n",
    "        P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "        #mm = [1 x output vocab size]\n",
    "        #P_over_extended_vocab = [1 x output vocab size] (element wise summation)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            if not (1 in p_duplicate_list[i]):\n",
    "                P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "        \n",
    "        '''\n",
    "        This above step makes sure if <UNK> token is the best decoder can produce over orginal \n",
    "        vocabulary, it is forced to look at extended vocab to produce the most appropriate word\n",
    "        '''\n",
    "        try: # Loss calculation\n",
    "            loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
    "            loss.backward(retain_graph=True)\n",
    "        except KeyError:\n",
    "            loss += torch.tensor(0,dtype=torch.float,device=device)\n",
    "    \n",
    "        if use_teacher_forcing:\n",
    "            '''if decoder is teacher forced, actual reference token (ground truth) is used as next decoder input'''\n",
    "            next_input = target_tensor[decoder_index]\n",
    "            if next_input.item() in ind2Word_dec.keys():\n",
    "                dec_train_word = ind2Word_dec[next_input.item()]\n",
    "                decoder_input = torch.tensor([word2PsuInd_dec[dec_train_word]], dtype=torch.long, device=device)\n",
    "            else:\n",
    "                decoder_input = torch.tensor([0], dtype=torch.long, device=device)\n",
    "            \n",
    "            if (decoder_input.item() == word2Index_dec['<END>']):\n",
    "                break       \n",
    "        else:\n",
    "            '''if not teacher forced, decoder output produced in the previous step is used as next decoder input'''\n",
    "            idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
    "            if idx.item() < len(word2Index_dec.keys()):   \n",
    "                decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
    "            elif idx.item() >= len(word2Index_dec.keys()):\n",
    "                #prev_unk_word = extended_vocab[idx.item()] # use <UNK> if doesn't work\n",
    "                decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
    "            \n",
    "            if (decoder_input.item() == word2Index_dec['<END>']):\n",
    "                break      \n",
    "\n",
    "\n",
    "    if iters > 20000:\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE4KJmThZd5C"
   },
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, input_tensor, target_tensor, criterion, max_length):\n",
    "    '''\n",
    "    Note: validate step is similar to train step but in the absence of Teacher forcing and varying gradient\n",
    "    Arg:\n",
    "        encoder: encoder model trained\n",
    "        decoder: decoder model trained\n",
    "        input_tensor: source seq in tensor form [seq length x 1] batch size = 1\n",
    "        target_tensor: target seq in tensor form [seq length x 1] batch size = 1\n",
    "        citerion: Loss criteria\n",
    "        max_length: maximum source length length desired\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        #prev_unk_word = ''\n",
    "\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        #encoder_hidden = [1 x 1 x hidden dim]\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "        #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        output_length = target_tensor.size(0)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for encoder_index in range(0, input_length):\n",
    "            encoder_output,encoder_hidden = encoder(input_tensor[encoder_index], encoder_hidden)\n",
    "            #input_tensor[encoder_index] = [1 x 1 x embed dim] (embed dim = hidden dim)\n",
    "            #encoder_hidden = [1 x 1 x hidden dim] {encoder arg inp}\n",
    "            #encoder_hidden = [n_layers*n direction x 1 x hidden dime] {encoder product}\n",
    "            #encoder_output = [seq length x 1 x hidden dim*n direction]\n",
    "            #seq length, n_layers, n direction = 1  \n",
    "\n",
    "            encoder_outputs[encoder_index] = encoder_output[0,0] # [1 x hidden dim]\n",
    "            #encoder_outputs: [seq length x hidden dim] ==> [seq length x hidden dim] (hidden state from all 0 to new)\n",
    "\n",
    "        decoder_input = torch.tensor([word2Index_dec['<START>']],device=device)   \n",
    "        #decoder_input = [1 x 1]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        #decoder_hidden = [1 x 1 x hidden dim]\n",
    "\n",
    "        extended_vocab = psuInd2Word_dec.copy()\n",
    "        reverse_extended_vocab = word2PsuInd_dec.copy()\n",
    "        duplicate_words = {}\n",
    "        extend_key = len(word2Index_dec.keys())\n",
    "        input_list = input_tensor.tolist()\n",
    "        i =0\n",
    "        for input_word in input_list:\n",
    "            if ind2Word_enc[input_word[0]] in word2Index_dec.keys():\n",
    "                duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word[0]]]\n",
    "            else:\n",
    "                extended_vocab[extend_key] = ind2Word_enc[input_word[0]]\n",
    "                reverse_extended_vocab[ind2Word_enc[input_word[0]]] = extend_key\n",
    "                extend_key += 1\n",
    "            i = i+1\n",
    "        \n",
    "        for decoder_index in range(output_length):\n",
    "            decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "            #decoder_input = [1 x 1]\n",
    "            #decoder_hidden = [1 x 1 x hidden dim]\n",
    "            #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "            #decoder_output = [1 x output vocab size]\n",
    "            #decoder_hidden = [1 x 1 x hidden dime]\n",
    "            #decoder_attention = [1 x max source length]\n",
    "            #pgen = [1 x 1]\n",
    "\n",
    "            P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "            #P_over_extended_vocab = [1 x output vocab size] (exp(decoder_output)*pgen)\n",
    "\n",
    "            decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "            #restricting decoder attention upto only input length\n",
    "            #decoder_attention = [1 x input_length]\n",
    "            p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "            #p_duplicate_list = [input_length x output vocab size] \n",
    "\n",
    "            p_duplicate_list = p_duplicate_list.tolist()\n",
    "            for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "                p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1 #making duplicate key,vals apparent\n",
    "\n",
    "            p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "            p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "            #p_diag = [1 x output vocab size]\n",
    "\n",
    "            p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "            #p_diag = p_diag*(1 - pgen)\n",
    "\n",
    "            p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0) #p_diag.squeeze(0) ==> [output vocab size]\n",
    "            #p_add_diag = [output vocab size x output vocab size]\n",
    "\n",
    "            P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "            #mm = [1 x output vocab size]\n",
    "            #P_over_extended_vocab = [1 x output vocab size] (element wise summation)\n",
    "\n",
    "            for i in range(input_length):\n",
    "                if not (1 in p_duplicate_list[i]):\n",
    "                    P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "\n",
    "            try:\n",
    "                loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
    "            except KeyError:\n",
    "                loss += torch.tensor(0,dtype=torch.float,device=device)\n",
    "\n",
    "            idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
    "            if idx.item() < len(word2Index_dec.keys()):   \n",
    "                decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
    "            elif idx.item() >= len(word2Index_dec.keys()):\n",
    "                #prev_unk_word = extended_vocab[idx.item()] # use <UNK> if doesn't work\n",
    "                decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
    "            if (decoder_input.item() == word2Index_dec['<END>']):\n",
    "                break      \n",
    "\n",
    "    return loss.item()/output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVoWvRycXU2T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "'''make directory for saving the model'''\n",
    "\n",
    "if not os.path.exists('checkpoints_table2text/encoder'):\n",
    "    os.makedirs('checkpoints_table2text/encoder')\n",
    "if not os.path.exists('checkpoints_table2text/decoder'):\n",
    "    os.makedirs('checkpoints_table2text/decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this section, we define the training procedure for our model. We choose the 'Stochastic Gradient Descent' `SGD` optimizer for both encoder and decoder and start off with very small learning rate as the size of dataset is very small in our case. Rest of the steps are self explanatory. The model is saved in the assigned directory when minimum validation loss occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBToTQxGLXgI"
   },
   "outputs": [],
   "source": [
    "def train_Iters(encoder,decoder,n_iters, print_every=1, plot_every=5,learning_rate = 0.0005):\n",
    "    '''\n",
    "    Args:\n",
    "        encoder: defined encoder\n",
    "        decoder: defined decoder\n",
    "        n_iters: int, number of iterations\n",
    "        print_every: int, prints the predicted text on a randomly selected example from\n",
    "                     training source every 'print_every' iterations\n",
    "        plot_every: int, stores the training loss every 'plot_every' iteration\n",
    "        learning rate: encoder and decoder optimizer's learning rate\n",
    "    '''\n",
    "    train_loss_graph = {}\n",
    "    val_loss_graph = {}\n",
    "    plot_losses = []\n",
    "    \n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0\n",
    "    print_val_loss = 0\n",
    "\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    encoder_input = [[word2Index_enc[word] if word in word2Index_enc.keys() else word2Index_enc['<UNK>'] for word in sentence.split()] for sentence in new_source ]\n",
    "    decoder_input = [[word2Index_dec_big[word] if word in word2Index_dec_big.keys() else word2Index_dec_big['<UNK>'] for word in sentence.split()] for sentence in new_summary ]\n",
    "    train_pairs = [[enc,dec] for enc,dec in zip(encoder_input,decoder_input)]\n",
    "    training_pairs = [random.choice(train_pairs) for i in range(n_iters)]\n",
    "\n",
    "    encoder_val = [[word2Index_enc[word] if word in word2Index_enc.keys() else word2Index_enc['<UNK>'] for word in sentence.split()] for sentence in test_source ]\n",
    "    decoder_val = [[word2Index_dec_big[word] if word in word2Index_dec_big.keys() else word2Index_dec_big['<UNK>'] for word in sentence.split()] for sentence in test_summary ]\n",
    "    val_pairs = [[enc,dec] for enc,dec in zip(encoder_val,decoder_val)]\n",
    "    validation_pairs = [random.choice(val_pairs) for i in range(n_iters)]\n",
    "\n",
    "    best_model_iters = []\n",
    "    best_valid_loss = float('inf')\n",
    "    criterion = nn.NLLLoss()\n",
    "    for iters in range(n_iters):\n",
    "        training_pair = training_pairs[iters]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        input_tensor = torch.tensor(input_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "        target_tensor = torch.tensor(target_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "\n",
    "        loss = train(encoder,decoder,input_tensor,target_tensor,\n",
    "                     encoder_optimizer,decoder_optimizer,criterion,max_source_length, iters=n_iters)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        validation_pair = validation_pairs[iters]\n",
    "        val_input_tensor = validation_pair[0]\n",
    "        val_target_tensor = validation_pair[1]\n",
    "\n",
    "        val_input_tensor = torch.tensor(val_input_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "        val_target_tensor = torch.tensor(val_target_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "\n",
    "        val_loss = validate(encoder, decoder, val_input_tensor, val_target_tensor, criterion, max_source_length)\n",
    "        print_val_loss += val_loss\n",
    "\n",
    "        if iters % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            val_loss_avg = print_val_loss / print_every\n",
    "            print_val_loss = 0\n",
    "\n",
    "            print(f'Iteration: {iters}, Train Loss: {print_loss_avg:.4f}, Val Loss: {val_loss_avg:.4f}') \n",
    "            evaluateRandomly(rnn_encoder, rnn_decoder, new_source, new_summary)\n",
    "            if iters > 0:\n",
    "                loss_graph[iters] = print_loss_avg\n",
    "\n",
    "        if iters % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "\n",
    "        if val_loss < best_valid_loss:\n",
    "            best_valid_loss = val_loss\n",
    "            torch.save(encoder, 'checkpoints_table2text/encoder/best_encoder.pt')\n",
    "            torch.save(decoder, 'checkpoints_table2text/decoder/best_decoder.pt')\n",
    "            best_model_iters.append(iters)\n",
    "\n",
    "    print(f'\\nbest model found at iteration {max(best_model_iters)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAJAoFoJDLrm"
   },
   "outputs": [],
   "source": [
    "'''Define the encoder and decoder model'''\n",
    "\n",
    "INPUT_VOCAB_SIZE = len(word2Index_enc.keys())\n",
    "OUTPUT_VOCAB_SIZE = len(word2Index_dec.keys())\n",
    "MAX_SOURCE_LENGTH = max_source_length\n",
    "DROPOUT_VAL = 0.2\n",
    "HIDDEN_SIZE = EMBED_DIM = 128\n",
    "\n",
    "rnn_encoder = Encoder(INPUT_VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE).to(device=device)\n",
    "rnn_decoder = AttentionDecoder(OUTPUT_VOCAB_SIZE, EMBED_DIM, \n",
    "                               HIDDEN_SIZE, MAX_SOURCE_LENGTH, DROPOUT_VAL).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObARxnAyoUTA"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, encoder_tensor, \n",
    "             max_source_length=max_source_length, max_summary_length=max_summary_length):\n",
    "    '''\n",
    "    returns a list decoded tokens on provided source tensor along with Attention tensor\n",
    "    \n",
    "    Args:\n",
    "        encoder: trained encoder\n",
    "        decoder: trained decoder\n",
    "        encoder_tensor: a LongTensor of sequence, dtype must be torch.long\n",
    "        max_source_length: int, maximum source length present in training set\n",
    "        max_summary_length: int, maximum target length present in the training set\n",
    "    '''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = encoder_tensor\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        #prev_unk_word = ''\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_source_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0),\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        extended_vocab = psuInd2Word_dec.copy()\n",
    "        duplicate_words = {}\n",
    "        extend_key = len(word2Index_dec.keys())\n",
    "        input_list = input_tensor.tolist()\n",
    "        i =0\n",
    "        for input_word in input_list:\n",
    "            if ind2Word_enc[input_word] in word2Index_dec.keys():\n",
    "                duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word]]\n",
    "            else:\n",
    "                extended_vocab[extend_key] = ind2Word_enc[input_word]\n",
    "                extend_key += 1\n",
    "            i = i+1\n",
    "\n",
    "        decoder_input = torch.tensor([word2Index_dec['<START>']], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_summary_length, max_source_length)\n",
    "\n",
    "        for di in range(max_summary_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention,pgen = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "\n",
    "            P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "\n",
    "            decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "            p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "            p_duplicate_list = p_duplicate_list.tolist()\n",
    "            for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "                p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1\n",
    "            p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "            p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "            p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "            p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0)\n",
    "            P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "\n",
    "            for i in range(input_length):\n",
    "                if not (1 in p_duplicate_list[i]):\n",
    "                    P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "\n",
    "            idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
    "            if idx.item() < len(word2Index_dec.keys()):   \n",
    "                decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
    "                decoded_words.append(extended_vocab[idx.item()])\n",
    "            elif idx.item() >= len(word2Index_dec.keys()):\n",
    "                decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
    "                prev_unk_word = extended_vocab[idx.item()] \n",
    "                decoded_words.append(prev_unk_word)\n",
    "            if idx.item() == word2Index_dec['<END>']:\n",
    "                decoded_words.append('<END>')\n",
    "                break\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-njk60qXU31"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, source_seqs, target_seqs=None, n=1):\n",
    "    '''\n",
    "    randomly selects a source sequence from a list of sequences and prints the decoded text\n",
    "    Args:\n",
    "        encoder: trained encoder model\n",
    "        decoder: trained decoder model\n",
    "        source_seqs: List of sequences in text form\n",
    "        target_seqs: List of corresponding target sequences in text form, default None\n",
    "        n: int, Number of random selections from soource_seqs, default 1\n",
    "    '''\n",
    "    for i in range(n):\n",
    "        idx = random.choice(range(len(source_seqs)))\n",
    "        source_seq = source_seqs[idx]\n",
    "        source_inp = [word2Index_enc[word] if word in word2Index_enc.keys() else word2Index_enc['<UNK>'] for word in source_seq.split()]      \n",
    "        source_tensor = torch.tensor(source_inp,dtype=torch.long,device=device)\n",
    "        output_words, attentions = evaluate(encoder, decoder, source_tensor)\n",
    "        output_seq = ' '.join(output_words)\n",
    "        \n",
    "        print('   SOURCE: ',source_seq)\n",
    "        if target_seqs is not None:\n",
    "            target_seq = target_seqs[idx]\n",
    "            print('   ACTUAL: ',target_seq)\n",
    "        print('PREDICTED: ',output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Train Loss: 0.5547, Val Loss: 0.5284\n",
      "   SOURCE:  name josiah gender male math 53 reading 44 writing 42\n",
      "   ACTUAL:  <START> josiah scores were really poor and he needs to put in a lot more effort in all three sections. <END>\n",
      "PREDICTED:  josiah 53 42 53 42 42 42 josiah 42 male 42 42 42 42 male 42 53 53 gender gender 53 42 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "F:\\Anaconda\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type AttentionDecoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10, Train Loss: 4.7632, Val Loss: 4.6299\n",
      "   SOURCE:  name lincoln gender male math 57 reading 56 writing 57\n",
      "   ACTUAL:  <START> lincoln scores were average at best and he should work harder in math.  <END>\n",
      "PREDICTED:  lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln lincoln\n",
      "Iteration: 20, Train Loss: 4.2701, Val Loss: 4.6575\n",
      "   SOURCE:  name hunter gender female math 33 reading 41 writing 43\n",
      "   ACTUAL:  <START> hunter failed in math and she will have to repeat the course, she nearly failed in other two sections as well. <END>\n",
      "PREDICTED:  hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter hunter\n",
      "Iteration: 30, Train Loss: 4.2032, Val Loss: 4.6274\n",
      "   SOURCE:  name elias gender male math 45 reading 37 writing 37\n",
      "   ACTUAL:  <START> elias nearly failed in both reading and writing and he barely passed in math. <END>\n",
      "PREDICTED:  elias elias elias elias elias elias elias elias elias in 45 elias elias elias in and elias elias elias elias in in 37\n",
      "Iteration: 40, Train Loss: 4.0793, Val Loss: 4.5922\n",
      "   SOURCE:  name jordan gender male math 49 reading 45 writing 45\n",
      "   ACTUAL:  <START> jordan performed poorely in the exam and he needs to put in extra hours in practice. <END>\n",
      "PREDICTED:  jordan jordan jordan jordan and and jordan jordan and jordan jordan jordan jordan jordan jordan jordan jordan jordan and and jordan and jordan\n",
      "Iteration: 50, Train Loss: 3.7708, Val Loss: 4.4285\n",
      "   SOURCE:  name jace gender female math 73 reading 86 writing 82\n",
      "   ACTUAL:  <START> jace performed really well in the exam and yet she can improve her math skills. <END>\n",
      "PREDICTED:  jace jace jace jace and and and and and and and and jace jace jace and and and and and jace jace and\n",
      "Iteration: 60, Train Loss: 4.0797, Val Loss: 4.3007\n",
      "   SOURCE:  name isaiah gender male math 53 reading 55 writing 48\n",
      "   ACTUAL:  <START> isaiah scores were average at best and he needs to work a lot on writing front. <END>\n",
      "PREDICTED:  <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> isaiah isaiah in in in in in in in in in in in\n",
      "Iteration: 70, Train Loss: 3.6736, Val Loss: 4.2876\n",
      "   SOURCE:  name robert gender female math 58 reading 63 writing 73\n",
      "   ACTUAL:  <START> robert scores were decent and she can improve if she practices more. <END>\n",
      "PREDICTED:  <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> <START> robert robert and and and and and and and and and\n",
      "Iteration: 80, Train Loss: 3.6578, Val Loss: 4.1192\n",
      "   SOURCE:  name ethan gender male math 40 reading 52 writing 43\n",
      "   ACTUAL:  <START> ethan performed really badly in the exams. <END>\n",
      "PREDICTED:  <START> ethan ethan and and and and and and and and and and and and and and and and and and and and\n",
      "Iteration: 90, Train Loss: 3.6909, Val Loss: 4.1468\n",
      "   SOURCE:  name jonathan gender male math 62 reading 61 writing 55\n",
      "   ACTUAL:  <START> jonathan's scores were average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> <START> jonathan jonathan in in in and and and and and and and and and and and and and and and and\n",
      "Iteration: 100, Train Loss: 3.6957, Val Loss: 4.1813\n",
      "   SOURCE:  name william gender female math 90 reading 95 writing 93\n",
      "   ACTUAL:  <START> william was one of the top performers in the class and she will do well in future. <END>\n",
      "PREDICTED:  <START> william william in in in in and and and and and and and and and and and and and and and and\n",
      "Iteration: 110, Train Loss: 3.4670, Val Loss: 4.3264\n",
      "   SOURCE:  name dominic gender male math 49 reading 49 writing 41\n",
      "   ACTUAL:  <START> dominic scores were poor and he barely managed to pass in writing. <END>\n",
      "PREDICTED:  <START> dominic dominic in in in in in in in in in in in in in in in in in in in in\n",
      "Iteration: 120, Train Loss: 3.6208, Val Loss: 4.2747\n",
      "   SOURCE:  name leo gender male math 82 reading 84 writing 82\n",
      "   ACTUAL:  <START> leo did really well in all three sections and he was remarkably consistent as well. <END>\n",
      "PREDICTED:  <START> leo leo scores and and and and and and and and and and and and and and and and and and and\n",
      "Iteration: 130, Train Loss: 3.2654, Val Loss: 4.3806\n",
      "   SOURCE:  name dominic gender male math 49 reading 49 writing 41\n",
      "   ACTUAL:  <START> dominic scores were poor and he barely managed to pass in writing. <END>\n",
      "PREDICTED:  <START> dominic scores scores and and and and and and and and and and and and and and and and and and and\n",
      "Iteration: 140, Train Loss: 3.0733, Val Loss: 4.2933\n",
      "   SOURCE:  name henry gender female math 69 reading 75 writing 78\n",
      "   ACTUAL:  <START> henry did alright but she can do better. <END>\n",
      "PREDICTED:  <START> henry henry scores were in and and and and and and and and and and and and and and and and and\n",
      "Iteration: 150, Train Loss: 3.3329, Val Loss: 4.4503\n",
      "   SOURCE:  name colton gender female math 60 reading 72 writing 74\n",
      "   ACTUAL:  <START> colton score decent in reading and writing but she can improve in math. <END>\n",
      "PREDICTED:  <START> colton scores in in in in in in in in in in in in in in in in in in in in\n",
      "Iteration: 160, Train Loss: 3.2194, Val Loss: 4.3914\n",
      "   SOURCE:  name noah gender female math 69 reading 90 writing 88\n",
      "   ACTUAL:  <START> noah scored good in reading and writing but her math score was an anomaly. <END>\n",
      "PREDICTED:  <START> noah scores did in and and and and and and and and and and and and and and and and and and\n",
      "Iteration: 170, Train Loss: 3.3481, Val Loss: 4.2517\n",
      "   SOURCE:  name angel gender male math 63 reading 55 writing 63\n",
      "   ACTUAL:  <START> angel performance was average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> angel did did and and and and and and and and and and and and and and and and and and and\n",
      "Iteration: 180, Train Loss: 2.9755, Val Loss: 4.2930\n",
      "   SOURCE:  name luke gender male math 70 reading 70 writing 65\n",
      "   ACTUAL:  <START> luke performance was decent but his writing can improve. <END>\n",
      "PREDICTED:  <START> luke did in and and and and and and and and and and and and and and and and and and and\n",
      "Iteration: 190, Train Loss: 3.1885, Val Loss: 4.6142\n",
      "   SOURCE:  name axel gender male math 43 reading 45 writing 50\n",
      "   ACTUAL:  <START> axel performed poorely in the exam and he needs to put in extra hours in practice. <END>\n",
      "PREDICTED:  <START> axel did in in in in in in in in in in in in in in in in in in in in\n",
      "Iteration: 200, Train Loss: 3.0050, Val Loss: 4.4891\n",
      "   SOURCE:  name josiah gender male math 53 reading 44 writing 42\n",
      "   ACTUAL:  <START> josiah scores were really poor and he needs to put in a lot more effort in all three sections. <END>\n",
      "PREDICTED:  <START> josiah did in in in in in in and and and and he in and and he in in in in in\n",
      "Iteration: 210, Train Loss: 3.1506, Val Loss: 4.5774\n",
      "   SOURCE:  name jack gender female math 67 reading 69 writing 75\n",
      "   ACTUAL:  <START> jack performance was decent and her writing has improved. <END>\n",
      "PREDICTED:  <START> jack scores were in in in in in and in and and he in and and he in in in and and\n",
      "Iteration: 220, Train Loss: 3.1881, Val Loss: 4.5134\n",
      "   SOURCE:  name henry gender female math 69 reading 75 writing 78\n",
      "   ACTUAL:  <START> henry did alright but she can do better. <END>\n",
      "PREDICTED:  <START> henry scored scored in in and and and and he and and he and he and he and he and he and\n",
      "Iteration: 230, Train Loss: 3.3046, Val Loss: 4.2753\n",
      "   SOURCE:  name sebastian gender female math 18 reading 32 writing 28\n",
      "   ACTUAL:  <START> sebastian was amongst the worst performers in the class and she failed the exams. <END>\n",
      "PREDICTED:  <START> sebastian scored in and and and and and and he and he and and he and he and he and and he\n",
      "Iteration: 240, Train Loss: 2.9304, Val Loss: 4.3825\n",
      "   SOURCE:  name robert gender female math 58 reading 63 writing 73\n",
      "   ACTUAL:  <START> robert scores were decent and she can improve if she practices more. <END>\n",
      "PREDICTED:  <START> robert scores were and and and he and he and he and he and he and he and he and he and\n",
      "Iteration: 250, Train Loss: 2.8874, Val Loss: 4.6281\n",
      "   SOURCE:  name nolan gender female math 69 reading 80 writing 71\n",
      "   ACTUAL:  <START> nolan scored alright and did well in reading but she can improve her maths. <END>\n",
      "PREDICTED:  <START> nolan scores were in and and he and he and he and he and he and he and he and he and\n",
      "Iteration: 260, Train Loss: 3.4946, Val Loss: 4.9020\n",
      "   SOURCE:  name anthony gender female math 50 reading 64 writing 59\n",
      "   ACTUAL:  <START> anthony scores were average at best and she should work harder in math.  <END>\n",
      "PREDICTED:  <START> anthony scores were in and and he and he and he and he and he and he and he and he and\n",
      "Iteration: 270, Train Loss: 2.9315, Val Loss: 4.5922\n",
      "   SOURCE:  name jonathan gender male math 62 reading 61 writing 55\n",
      "   ACTUAL:  <START> jonathan's scores were average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> jonathan scores were were and and and he and he and he and he and he and he and he and he\n",
      "Iteration: 280, Train Loss: 3.3298, Val Loss: 4.5641\n",
      "   SOURCE:  name cameron gender male math 61 reading 58 writing 56\n",
      "   ACTUAL:  <START> cameron scores were average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> cameron scored average in and and and and he and and he and he in and and and he and and he\n",
      "Iteration: 290, Train Loss: 3.3608, Val Loss: 5.0485\n",
      "   SOURCE:  name landon gender male math 79 reading 74 writing 72\n",
      "   ACTUAL:  <START> landon did decent and with slightly more effort he could be a top student. <END>\n",
      "PREDICTED:  <START> landon scores were average and in and and he in and and he in and and he in and and he in\n",
      "Iteration: 300, Train Loss: 3.2677, Val Loss: 4.7064\n",
      "   SOURCE:  name wyatt gender male math 73 reading 74 writing 72\n",
      "   ACTUAL:  <START> wyatt's performance was decent and he was consistent through out. <END>\n",
      "PREDICTED:  <START> wyatt performed were in and and he and he to in and he and he to in and he and he needs\n",
      "Iteration: 310, Train Loss: 2.8462, Val Loss: 4.4998\n",
      "   SOURCE:  name benjamin gender female math 71 reading 83 writing 78\n",
      "   ACTUAL:  <START> benjamin was consistent and with more efforts she could have done well. <END>\n",
      "PREDICTED:  <START> benjamin performed performed in and and he in and he in and he in and he in and he in and he\n",
      "Iteration: 320, Train Loss: 3.2090, Val Loss: 4.9217\n",
      "   SOURCE:  name eli gender male math 52 reading 55 writing 49\n",
      "   ACTUAL:  <START> eli scores were average and he needs to work a lot in all three sections. <END>\n",
      "PREDICTED:  <START> eli scores were average and and he and he to in and he and he to in and he he needs and\n",
      "Iteration: 330, Train Loss: 3.2769, Val Loss: 4.6025\n",
      "   SOURCE:  name david gender female math 65 reading 75 writing 70\n",
      "   ACTUAL:  <START> david did alright but she can do better. <END>\n",
      "PREDICTED:  <START> david scores were average and he he to in and he to in and he to in and he to in and\n",
      "Iteration: 340, Train Loss: 2.6779, Val Loss: 4.5999\n",
      "   SOURCE:  name sebastian gender female math 18 reading 32 writing 28\n",
      "   ACTUAL:  <START> sebastian was amongst the worst performers in the class and she failed the exams. <END>\n",
      "PREDICTED:  <START> sebastian did were in and he he to to in and he to to in and he to to in and he\n",
      "Iteration: 350, Train Loss: 3.0569, Val Loss: 4.7180\n",
      "   SOURCE:  name jose gender female math 58 reading 70 writing 67\n",
      "   ACTUAL:  <START> jose performed alright but her math score was below average. <END>\n",
      "PREDICTED:  <START> jose was in in and and he to to in and she and to to in and he to to in and\n",
      "Iteration: 360, Train Loss: 3.0324, Val Loss: 4.9971\n",
      "   SOURCE:  name bryson gender male math 71 reading 79 writing 71\n",
      "   ACTUAL:  <START> bryson did well in the all three sections and his reading has improved. <END>\n",
      "PREDICTED:  <START> bryson was was in and and he she to to in and she to to in and and to to to in\n",
      "Iteration: 370, Train Loss: 3.2015, Val Loss: 5.2607\n",
      "   SOURCE:  name thomas gender female math 57 reading 74 writing 76\n",
      "   ACTUAL:  <START> thomas performed well in reading and writing but her math score was average. <END>\n",
      "PREDICTED:  <START> thomas scores were in and and he and he to to in and he and he to in and he and he\n",
      "Iteration: 380, Train Loss: 3.0320, Val Loss: 4.8278\n",
      "   SOURCE:  name santiago gender female math 47 reading 49 writing 50\n",
      "   ACTUAL:  <START> santiago performed poorely in the exam and she really needs to put in longer hours. <END>\n",
      "PREDICTED:  <START> santiago scores were in and and he to in and he needs to to in and he to to in and he\n",
      "Iteration: 390, Train Loss: 3.2883, Val Loss: 5.6010\n",
      "   SOURCE:  name theodore gender male math 59 reading 65 writing 66\n",
      "   ACTUAL:  <START> theodore scored average marks and there is scope for improvement in her math skills. <END>\n",
      "PREDICTED:  <START> theodore scores were in and and he needs to to to in and he needs to to in and he needs to\n",
      "Iteration: 400, Train Loss: 3.2946, Val Loss: 5.1581\n",
      "   SOURCE:  name josiah gender male math 53 reading 44 writing 42\n",
      "   ACTUAL:  <START> josiah scores were really poor and he needs to put in a lot more effort in all three sections. <END>\n",
      "PREDICTED:  <START> josiah performed was in and and he needs to in and he in and needs to in and he in and he\n",
      "Iteration: 410, Train Loss: 2.8518, Val Loss: 5.3727\n",
      "   SOURCE:  name sawyer gender female math 58 reading 67 writing 62\n",
      "   ACTUAL:  <START> sawyer did alright but she has to work on her math skills. <END>\n",
      "PREDICTED:  <START> sawyer did decent in and and he he to in and he to in and he to to in and he to\n",
      "Iteration: 420, Train Loss: 2.7893, Val Loss: 5.2173\n",
      "   SOURCE:  name leonardo gender male math 27 reading 34 writing 36\n",
      "   ACTUAL:  <START> leonardo failed in the exam and he will have to repeat the course. <END>\n",
      "PREDICTED:  <START> leonardo performance decent in and and she she to to in and she she to in and she to in and she\n",
      "Iteration: 430, Train Loss: 2.7427, Val Loss: 5.3902\n",
      "   SOURCE:  name eli gender male math 52 reading 55 writing 49\n",
      "   ACTUAL:  <START> eli scores were average and he needs to work a lot in all three sections. <END>\n",
      "PREDICTED:  <START> eli did decent in and and she to to to in and she to to to in and she to to to\n",
      "Iteration: 440, Train Loss: 2.7648, Val Loss: 5.2376\n",
      "   SOURCE:  name gabriel gender male math 97 reading 87 writing 82\n",
      "   ACTUAL:  <START> gabriel scored highest marks in maths and he could have done better in writing. <END>\n",
      "PREDICTED:  <START> gabriel did decent in and and he to to to to to to to in in and he to to to to\n",
      "Iteration: 450, Train Loss: 3.1707, Val Loss: 5.5898\n",
      "   SOURCE:  name colton gender female math 60 reading 72 writing 74\n",
      "   ACTUAL:  <START> colton score decent in reading and writing but she can improve in math. <END>\n",
      "PREDICTED:  <START> colton performed in in and and she and she to to to to to to to to in and she to to\n",
      "Iteration: 460, Train Loss: 2.8715, Val Loss: 5.5512\n",
      "   SOURCE:  name xavier gender female math 71 reading 71 writing 74\n",
      "   ACTUAL:  <START> xavier scores were decent in all three sections. <END>\n",
      "PREDICTED:  <START> xavier was was in and and she she to to to to in and she to to to in and she to\n",
      "Iteration: 470, Train Loss: 2.8677, Val Loss: 5.5653\n",
      "   SOURCE:  name theodore gender male math 59 reading 65 writing 66\n",
      "   ACTUAL:  <START> theodore scored average marks and there is scope for improvement in her math skills. <END>\n",
      "PREDICTED:  <START> theodore was in the and and he she to to in and he to in and he to to in the and\n",
      "Iteration: 480, Train Loss: 3.1668, Val Loss: 5.3378\n",
      "   SOURCE:  name henry gender female math 69 reading 75 writing 78\n",
      "   ACTUAL:  <START> henry did alright but she can do better. <END>\n",
      "PREDICTED:  <START> henry did decent in the and she and she to to in the and she to in the and she she in\n",
      "Iteration: 490, Train Loss: 2.7967, Val Loss: 5.1232\n",
      "   SOURCE:  name owen gender male math 74 reading 71 writing 80\n",
      "   ACTUAL:  <START> owen performance was decent and his writing has improved. <END>\n",
      "PREDICTED:  <START> owen did decent in the and she she to to in and she to in the and she to in the and\n",
      "Iteration: 500, Train Loss: 2.6787, Val Loss: 4.9795\n",
      "   SOURCE:  name william gender female math 90 reading 95 writing 93\n",
      "   ACTUAL:  <START> william was one of the top performers in the class and she will do well in future. <END>\n",
      "PREDICTED:  <START> william did decent in and and he he to to to in and he to to to in and he to to\n",
      "Iteration: 510, Train Loss: 3.0168, Val Loss: 5.0927\n",
      "   SOURCE:  name james gender male math 47 reading 57 writing 44\n",
      "   ACTUAL:  <START> james performed poorly across all three subjects and he needs to put in extra efforts to improve. <END>\n",
      "PREDICTED:  <START> james did were and and and he to in and to in and she to in and to in and he to\n",
      "Iteration: 520, Train Loss: 2.9879, Val Loss: 5.1810\n",
      "   SOURCE:  name landon gender male math 79 reading 74 writing 72\n",
      "   ACTUAL:  <START> landon did decent and with slightly more effort he could be a top student. <END>\n",
      "PREDICTED:  <START> landon scored average and and and she to in and she to in and she to to in and she to in\n",
      "Iteration: 530, Train Loss: 3.1117, Val Loss: 5.6768\n",
      "   SOURCE:  name cameron gender male math 61 reading 58 writing 56\n",
      "   ACTUAL:  <START> cameron scores were average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> cameron scores were average and and he and he to in in and he to in and to to in and to\n",
      "Iteration: 540, Train Loss: 2.8742, Val Loss: 5.7487\n",
      "   SOURCE:  name william gender female math 90 reading 95 writing 93\n",
      "   ACTUAL:  <START> william was one of the top performers in the class and she will do well in future. <END>\n",
      "PREDICTED:  <START> william scores were average and and he in and he in and he in and he in and he in and he\n",
      "Iteration: 550, Train Loss: 2.1473, Val Loss: 5.7508\n",
      "   SOURCE:  name robert gender female math 58 reading 63 writing 73\n",
      "   ACTUAL:  <START> robert scores were decent and she can improve if she practices more. <END>\n",
      "PREDICTED:  <START> robert scored average and and he in and he in and he in and he in and he in and he in\n",
      "Iteration: 560, Train Loss: 2.7727, Val Loss: 5.4197\n",
      "   SOURCE:  name jace gender female math 73 reading 86 writing 82\n",
      "   ACTUAL:  <START> jace performed really well in the exam and yet she can improve her math skills. <END>\n",
      "PREDICTED:  <START> jace did average and and he in and and in and in and in and in and in and and in and\n",
      "Iteration: 570, Train Loss: 2.7919, Val Loss: 5.4368\n",
      "   SOURCE:  name matthew gender female math 54 reading 58 writing 61\n",
      "   ACTUAL:  <START> matthew was average but she was consistent. <END>\n",
      "PREDICTED:  <START> matthew scored average in and and he in and he in and in and in and he in and in and in\n",
      "Iteration: 580, Train Loss: 2.9874, Val Loss: 5.4638\n",
      "   SOURCE:  name joseph gender male math 44 reading 54 writing 53\n",
      "   ACTUAL:  <START> joseph scores were very subpar and he needs to practice more. <END>\n",
      "PREDICTED:  <START> joseph did were in and and and he and to in and and to in and he to in and and he\n",
      "Iteration: 590, Train Loss: 3.2067, Val Loss: 5.5349\n",
      "   SOURCE:  name jordan gender male math 49 reading 45 writing 45\n",
      "   ACTUAL:  <START> jordan performed poorely in the exam and he needs to put in extra hours in practice. <END>\n",
      "PREDICTED:  <START> jordan did were in and and and he in and in and in and in and and in and in and in\n",
      "Iteration: 600, Train Loss: 2.9721, Val Loss: 5.3029\n",
      "   SOURCE:  name kayden gender male math 65 reading 66 writing 62\n",
      "   ACTUAL:  <START> kayden did alright but he has to work on her writing skills and practice more. <END>\n",
      "PREDICTED:  <START> kayden was was decent and and and he and to in and he to in and and to in and and he\n",
      "Iteration: 610, Train Loss: 2.9771, Val Loss: 6.2573\n",
      "   SOURCE:  name jordan gender male math 49 reading 45 writing 45\n",
      "   ACTUAL:  <START> jordan performed poorely in the exam and he needs to put in extra hours in practice. <END>\n",
      "PREDICTED:  <START> jordan scores were in and and and he to in and she to in and she to in and she to in\n",
      "Iteration: 620, Train Loss: 2.6290, Val Loss: 6.8280\n",
      "   SOURCE:  name samuel gender male math 66 reading 69 writing 63\n",
      "   ACTUAL:  <START> samuel did alright but he can do better. <END>\n",
      "PREDICTED:  <START> samuel scored were in and and he she to to in and she to in and she to in and she to\n",
      "Iteration: 630, Train Loss: 2.7095, Val Loss: 6.3059\n",
      "   SOURCE:  name cameron gender male math 61 reading 58 writing 56\n",
      "   ACTUAL:  <START> cameron scores were average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> cameron was in in and he he she to in in the and he she to in in the and he to\n",
      "Iteration: 640, Train Loss: 2.8039, Val Loss: 6.3060\n",
      "   SOURCE:  name joshua gender male math 55 reading 61 writing 54\n",
      "   ACTUAL:  <START> joshua scores were average at best and he should work harder in math and writing.  <END>\n",
      "PREDICTED:  <START> joshua did were in and and he he he to to in in and he he to to in in and he\n",
      "Iteration: 650, Train Loss: 2.7482, Val Loss: 6.2575\n",
      "   SOURCE:  name william gender female math 90 reading 95 writing 93\n",
      "   ACTUAL:  <START> william was one of the top performers in the class and she will do well in future. <END>\n",
      "PREDICTED:  <START> william scores were in and and he he he to to in in and he he to in in and he he\n",
      "Iteration: 660, Train Loss: 2.8769, Val Loss: 5.4671\n",
      "   SOURCE:  name david gender female math 65 reading 75 writing 70\n",
      "   ACTUAL:  <START> david did alright but she can do better. <END>\n",
      "PREDICTED:  <START> david scores were average and and he he he he to to to to in in and he he he to to\n",
      "Iteration: 670, Train Loss: 2.8010, Val Loss: 6.2742\n",
      "   SOURCE:  name brayden gender male math 72 reading 64 writing 63\n",
      "   ACTUAL:  <START> brayden did decent in all three sections and scores shows that he has improved in math.  <END>\n",
      "PREDICTED:  <START> brayden scores were in and and he to to to to in and she to to to in in and she to\n",
      "Iteration: 680, Train Loss: 2.1942, Val Loss: 5.5559\n",
      "   SOURCE:  name thomas gender female math 57 reading 74 writing 76\n",
      "   ACTUAL:  <START> thomas performed well in reading and writing but her math score was average. <END>\n",
      "PREDICTED:  <START> thomas was was in the and she she to to in in the and she to to in in the and she\n",
      "Iteration: 690, Train Loss: 3.0946, Val Loss: 6.2611\n",
      "   SOURCE:  name matthew gender female math 54 reading 58 writing 61\n",
      "   ACTUAL:  <START> matthew was average but she was consistent. <END>\n",
      "PREDICTED:  <START> matthew scores were average and he and he to to in in and she to in in and she to to in\n",
      "Iteration: 700, Train Loss: 3.1429, Val Loss: 6.4972\n",
      "   SOURCE:  name samuel gender male math 66 reading 69 writing 63\n",
      "   ACTUAL:  <START> samuel did alright but he can do better. <END>\n",
      "PREDICTED:  <START> samuel scores were in and and she she to to in in in and she to in in in and she to\n",
      "Iteration: 710, Train Loss: 2.8968, Val Loss: 6.7261\n",
      "   SOURCE:  name caleb gender female math 50 reading 56 writing 54\n",
      "   ACTUAL:  <START> caleb scores were average at best and she should work harder in all three sections <END>\n",
      "PREDICTED:  <START> caleb was decent in and and she she to in in the and she to in in the and she to in\n",
      "Iteration: 720, Train Loss: 2.9395, Val Loss: 6.1892\n",
      "   SOURCE:  name jackson gender male math 88 reading 89 writing 86\n",
      "   ACTUAL:  <START> jackson was one of the best scorers in the class. <END>\n",
      "PREDICTED:  <START> jackson was were average and and she she to in she in the was in the and she she in she in\n",
      "Iteration: 730, Train Loss: 2.8811, Val Loss: 6.8900\n",
      "   SOURCE:  name sebastian gender female math 18 reading 32 writing 28\n",
      "   ACTUAL:  <START> sebastian was amongst the worst performers in the class and she failed the exams. <END>\n",
      "PREDICTED:  <START> sebastian was were in and and she she in and she in the was in the and she in the and she\n",
      "Iteration: 740, Train Loss: 2.4443, Val Loss: 6.9114\n",
      "   SOURCE:  name jose gender female math 58 reading 70 writing 67\n",
      "   ACTUAL:  <START> jose performed alright but her math score was below average. <END>\n",
      "PREDICTED:  <START> jose was were average and he he he to in in the and he she in the in the and he he\n",
      "Iteration: 750, Train Loss: 3.0078, Val Loss: 7.3676\n",
      "   SOURCE:  name ethan gender male math 40 reading 52 writing 43\n",
      "   ACTUAL:  <START> ethan performed really badly in the exams. <END>\n",
      "PREDICTED:  <START> ethan did were in and and she she in and she in in in the and she she in in and she\n",
      "Iteration: 760, Train Loss: 2.6193, Val Loss: 7.0044\n",
      "   SOURCE:  name mateo gender female math 74 reading 81 writing 83\n",
      "   ACTUAL:  <START> mateo scores were good and she was very consistent too. <END>\n",
      "PREDICTED:  <START> mateo did were in and and she in and she in in the was in the and was in in the and\n",
      "Iteration: 770, Train Loss: 2.8973, Val Loss: 7.1216\n",
      "   SOURCE:  name santiago gender female math 47 reading 49 writing 50\n",
      "   ACTUAL:  <START> santiago performed poorely in the exam and she really needs to put in longer hours. <END>\n",
      "PREDICTED:  <START> santiago scores were average and and and she in in and she in in in in in and she in in in\n",
      "Iteration: 780, Train Loss: 2.7539, Val Loss: 7.5084\n",
      "   SOURCE:  name asher gender female math 55 reading 65 writing 62\n",
      "   ACTUAL:  <START> asher scored average marks and there is scope for improvement in her math skills. <END>\n",
      "PREDICTED:  <START> asher scores were average and and she in in in in in in in and she in in in in in in\n",
      "Iteration: 790, Train Loss: 2.8198, Val Loss: 7.2963\n",
      "   SOURCE:  name cooper gender male math 50 reading 47 writing 54\n",
      "   ACTUAL:  <START> cooper scores were below average and he needs to up his performance. <END>\n",
      "PREDICTED:  <START> cooper scores were average and and she she needs to in in in in in in she in in <END> she in\n",
      "Iteration: 800, Train Loss: 2.9289, Val Loss: 7.5854\n",
      "   SOURCE:  name connor gender female math 82 reading 85 writing 86\n",
      "   ACTUAL:  <START> connor exhibited a top performance and she was very consistent as well. <END>\n",
      "PREDICTED:  <START> connor did were average and and she and she to in in in in in the in in the in the and\n",
      "Iteration: 810, Train Loss: 2.8246, Val Loss: 7.4105\n",
      "   SOURCE:  name austin gender female math 62 reading 68 writing 68\n",
      "   ACTUAL:  <START> austin did alright and there is an scope of improvement for her. <END>\n",
      "PREDICTED:  <START> austin was decent in and she she and she she to in in in in in in in in the in the\n",
      "Iteration: 820, Train Loss: 2.8459, Val Loss: 7.0767\n",
      "   SOURCE:  name brayden gender male math 72 reading 64 writing 63\n",
      "   ACTUAL:  <START> brayden did decent in all three sections and scores shows that he has improved in math.  <END>\n",
      "PREDICTED:  <START> brayden did were decent and and she she she to in in in in in in in in the in in the\n",
      "Iteration: 830, Train Loss: 2.6091, Val Loss: 6.7632\n",
      "   SOURCE:  name colton gender female math 60 reading 72 writing 74\n",
      "   ACTUAL:  <START> colton score decent in reading and writing but she can improve in math. <END>\n",
      "PREDICTED:  <START> colton scores were average and she and she she she in in in in in in in in in in in in\n",
      "Iteration: 840, Train Loss: 2.6408, Val Loss: 7.5065\n",
      "   SOURCE:  name axel gender male math 43 reading 45 writing 50\n",
      "   ACTUAL:  <START> axel performed poorely in the exam and he needs to put in extra hours in practice. <END>\n",
      "PREDICTED:  <START> axel was decent in and he he he to to in in in all in in in in all in in in\n",
      "Iteration: 850, Train Loss: 2.7611, Val Loss: 6.4208\n",
      "   SOURCE:  name gabriel gender male math 97 reading 87 writing 82\n",
      "   ACTUAL:  <START> gabriel scored highest marks in maths and he could have done better in writing. <END>\n",
      "PREDICTED:  <START> gabriel scored decent in in all and she she needs to to to to to in in in in all in in\n",
      "Iteration: 860, Train Loss: 2.8040, Val Loss: 7.3107\n",
      "   SOURCE:  name cameron gender male math 61 reading 58 writing 56\n",
      "   ACTUAL:  <START> cameron scores were average and he can improve if he practices more. <END>\n",
      "PREDICTED:  <START> cameron did were average and he he needs to to to to in <END> in <END> in all three and he needs\n",
      "Iteration: 870, Train Loss: 2.8191, Val Loss: 6.9048\n",
      "   SOURCE:  name jackson gender male math 88 reading 89 writing 86\n",
      "   ACTUAL:  <START> jackson was one of the best scorers in the class. <END>\n",
      "PREDICTED:  <START> jackson was average in the and she and she she she needs to to in in all three in all three in\n",
      "Iteration: 880, Train Loss: 2.9110, Val Loss: 6.8389\n",
      "   SOURCE:  name joseph gender male math 44 reading 54 writing 53\n",
      "   ACTUAL:  <START> joseph scores were very subpar and he needs to practice more. <END>\n",
      "PREDICTED:  <START> joseph scores were average and and he he to in in <END> <END> in <END> in in in all in in all\n",
      "Iteration: 890, Train Loss: 2.7311, Val Loss: 7.6472\n",
      "   SOURCE:  name lincoln gender male math 57 reading 56 writing 57\n",
      "   ACTUAL:  <START> lincoln scores were average at best and he should work harder in math.  <END>\n",
      "PREDICTED:  <START> lincoln scored average in and he and he needs to to in in in in in in all in in all three\n",
      "Iteration: 900, Train Loss: 2.7850, Val Loss: 7.8727\n",
      "   SOURCE:  name theodore gender male math 59 reading 65 writing 66\n",
      "   ACTUAL:  <START> theodore scored average marks and there is scope for improvement in her math skills. <END>\n",
      "PREDICTED:  <START> theodore was average and he and he he needs to to in in in all in in all three in in all\n",
      "Iteration: 910, Train Loss: 2.7627, Val Loss: 7.6052\n",
      "   SOURCE:  name carson gender female math 39 reading 64 writing 57\n",
      "   ACTUAL:  <START> carson performed poorly in maths where she barely passed. <END>\n",
      "PREDICTED:  <START> carson did were average and and he he needs to to in in <END> in <END> in in all in all three\n",
      "Iteration: 920, Train Loss: 2.7079, Val Loss: 7.1597\n",
      "   SOURCE:  name benjamin gender female math 71 reading 83 writing 78\n",
      "   ACTUAL:  <START> benjamin was consistent and with more efforts she could have done well. <END>\n",
      "PREDICTED:  <START> benjamin did alright in all and he and he to to in in <END> in <END> in <END> in <END> in <END>\n",
      "Iteration: 930, Train Loss: 3.0889, Val Loss: 7.4858\n",
      "   SOURCE:  name axel gender male math 43 reading 45 writing 50\n",
      "   ACTUAL:  <START> axel performed poorely in the exam and he needs to put in extra hours in practice. <END>\n",
      "PREDICTED:  <START> axel did alright in the and and he and he in in <END> in in <END> in in in <END> in in\n",
      "Iteration: 940, Train Loss: 2.9499, Val Loss: 7.2417\n",
      "   SOURCE:  name isaiah gender male math 53 reading 55 writing 48\n",
      "   ACTUAL:  <START> isaiah scores were average at best and he needs to work a lot on writing front. <END>\n",
      "PREDICTED:  <START> isaiah was alright in the and he and he and he needs to in in <END> in <END> in in <END> in\n",
      "Iteration: 950, Train Loss: 2.3664, Val Loss: 7.8053\n",
      "   SOURCE:  name brayden gender male math 72 reading 64 writing 63\n",
      "   ACTUAL:  <START> brayden did decent in all three sections and scores shows that he has improved in math.  <END>\n",
      "PREDICTED:  <START> brayden did were average and and he and he he to in <END> <END> in <END> in <END> in <END> <END> in\n",
      "Iteration: 960, Train Loss: 2.9057, Val Loss: 7.9214\n",
      "   SOURCE:  name levi gender female math 56 reading 72 writing 65\n",
      "   ACTUAL:  <START> levi scored decent in reading but her performance was average in math. <END>\n",
      "PREDICTED:  <START> levi scores were in and and he and he to in <END> in <END> in <END> in <END> in <END> in in\n",
      "Iteration: 970, Train Loss: 2.7898, Val Loss: 7.6204\n",
      "   SOURCE:  name santiago gender female math 47 reading 49 writing 50\n",
      "   ACTUAL:  <START> santiago performed poorely in the exam and she really needs to put in longer hours. <END>\n",
      "PREDICTED:  <START> santiago scores were average and and he and he to in in in <END> in in in <END> in <END> in in\n",
      "Iteration: 980, Train Loss: 2.6341, Val Loss: 7.6331\n",
      "   SOURCE:  name lincoln gender male math 57 reading 56 writing 57\n",
      "   ACTUAL:  <START> lincoln scores were average at best and he should work harder in math.  <END>\n",
      "PREDICTED:  <START> lincoln scores were average and and he and he to to in in in in in <END> in in in in in\n",
      "Iteration: 990, Train Loss: 2.7254, Val Loss: 7.0081\n",
      "   SOURCE:  name daniel gender female math 50 reading 53 writing 58\n",
      "   ACTUAL:  <START> daniel scored average marks in all three section and she needs to work more on her maths. <END>\n",
      "PREDICTED:  <START> daniel did were average and and he and he to in in in in in <END> in in in in in in\n",
      "\n",
      "best model found at iteration 78\n"
     ]
    }
   ],
   "source": [
    "train_Iters(rnn_encoder,rnn_decoder,1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PointerGenerator_1stRun.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
