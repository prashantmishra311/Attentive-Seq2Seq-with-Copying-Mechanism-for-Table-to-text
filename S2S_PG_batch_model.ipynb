{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'C:\\Users\\hp\\Desktop\\WikiTable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QYoIaaQHTCaT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, random, os, torch, math, time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import Field, BucketIterator, get_tokenizer, Dataset, TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda text: text.split()\n",
    "\n",
    "ATTRIBUTE = Field(tokenize=tokenize)\n",
    "CELL = Field(tokenize=tokenize)\n",
    "CAPTION = Field(tokenize=tokenize, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('attributes',ATTRIBUTE),('cells',CELL),('captions',CAPTION)]\n",
    "\n",
    "train_data, val_data, test_data = TabularDataset.splits(path=filepath, \n",
    "                                                        format='csv',\n",
    "                                                        train='train.csv', \n",
    "                                                        validation='val.csv', \n",
    "                                                        test='test.csv', \n",
    "                                                        skip_header=True, \n",
    "                                                        fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 10000\n",
      "Number of validation examples: 1318\n",
      "Number of testing examples: 2000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attributes': ['subj_title', 'subj_subtitle', 'date', 'winning_$$_team', 'score', 'losing_$$_team'], 'cells': ['1978_$$_federation_$$_cup_$$_(_$$_tennis_$$_)', 'qualifying_$$_round', '19_$$_august', 'philippines', '3–0', 'thailand'], 'captions': ['philippines', 'won', 'thailand', 'with', '3–0', 'during', '1978', 'federation', 'cup.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTRIBUTE.build_vocab(train_data, min_freq = 1)\n",
    "CELL.build_vocab(train_data, min_freq = 1)\n",
    "CAPTION.build_vocab(train_data, min_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in attributes vocabulary: 2905\n",
      "Unique tokens in cells vocabulary: 29688\n",
      "Unique tokens in captions vocabulary: 25176\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in attributes vocabulary: {len(ATTRIBUTE.vocab)}\")\n",
    "print(f\"Unique tokens in cells vocabulary: {len(CELL.vocab)}\")\n",
    "print(f\"Unique tokens in captions vocabulary: {len(CAPTION.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, val_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training batch attributes: torch.Size([12, 128])\n",
      "shape of training batch cells: torch.Size([12, 128])\n",
      "shape of training batch captions: torch.Size([22, 128])\n"
     ]
    }
   ],
   "source": [
    "train_batch = next(iter(train_iterator))\n",
    "\n",
    "print(f'shape of training batch attributes: {train_batch.attributes.shape}')\n",
    "print(f'shape of training batch cells: {train_batch.cells.shape}')\n",
    "print(f'shape of training batch captions: {train_batch.captions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RUaNE15DpNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum attribute length:  12\n",
      "Maximum source length is:  12\n",
      "Maximum target length is:  47\n"
     ]
    }
   ],
   "source": [
    "max_attr_length = max([len(train_data[i].attributes) for i in range(len(train_data))])\n",
    "max_source_length = max([len(train_data[i].cells) for i in range(len(train_data))])\n",
    "max_summary_length = max([len(train_data[i].captions) for i in range(len(train_data))])\n",
    "print('Maximum attribute length: ', max_attr_length)\n",
    "print('Maximum source length is: ', max_source_length)\n",
    "print('Maximum target length is: ', max_summary_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_cell, id2word_cell = {}, {}\n",
    "for word,idx in CELL.vocab.stoi.items():\n",
    "    word2id_cell[word] = idx\n",
    "    id2word_cell[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id_cap, id2word_cap = {}, {}\n",
    "for word,idx in CAPTION.vocab.stoi.items():\n",
    "    word2id_cap[word] = idx\n",
    "    id2word_cap[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERJFNd_0XU1u"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \n",
    "  '''\n",
    "  Args:\n",
    "    input_vocab_size: (int) Size of source vocabulary\n",
    "    embed_size: (int) Embedding dimensions\n",
    "    hidden_size: (int) Dimensions of hidden state\n",
    "    num_layers: (int) Number of stacked GRU layers, default is 1\n",
    "    bidirectional: (Bool) If RNN is required to be birectional in nature, default is False\n",
    "  '''\n",
    "  \n",
    "  def __init__(self,attr_vocab_size, input_vocab_size, embed_size, hidden_size,num_layers=1,bidirectional=False):\n",
    "    super(Encoder,self).__init__()\n",
    "\n",
    "    self.bidirectional = bidirectional\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.hidden_size = hidden_size\n",
    "    self.attr_vocab_size = attr_vocab_size\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "\n",
    "    self.embedding = nn.Embedding(input_vocab_size, embed_size) #new try with embedding\n",
    "    self.linear = nn.Linear(2*embed_size, embed_size)\n",
    "    self.nonlinear = nn.Tanh()\n",
    "\n",
    "    self.gru_layer = nn.GRU(embed_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "\n",
    "  def forward(self, input_cell, input_attr, prev_hidden_state):\n",
    "    '''Arg:\n",
    "        input_: Tensor of source word indices\n",
    "        prev_hidden_state: Previous hidden state\n",
    "    '''\n",
    "\n",
    "    attr_tensor = input_attr\n",
    "    #cell_tensor = [seq length x batch size]\n",
    "    cell_tensor = input_cell\n",
    "    #cell_tensor = [seq length x batch size]\n",
    "    embedded_attr = self.embedding(attr_tensor)\n",
    "    #embedded_attr = [seq length x batch size x embed dim]\n",
    "    embedded_cell = self.embedding(cell_tensor)\n",
    "    #embedded_cell = [seq length x batch size x embed dim]\n",
    "    input_source = torch.cat((embedded_cell,embedded_attr), -1)\n",
    "    #input_source = [seq length x batch size x 2*embed dim]\n",
    "    \n",
    "    input_tanh = self.linear(input_source)\n",
    "    embedded_outputs = self.nonlinear(input_tanh)\n",
    "\n",
    "    output, prev_hidden_state = self.gru_layer(embedded_outputs,prev_hidden_state)  #output is batch_size times hidden_size\n",
    "    #prev_hidden_state = [n_layers*n direction x batch size x hidden dime]\n",
    "    #output = [seq length x batch size x hidden dim*n direction]\n",
    "    # default n direction = 1\n",
    "    #outputs are always from the top hidden layer\n",
    "    return output,prev_hidden_state\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return torch.zeros(1,1,self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I29Rw8CA7esY"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "\n",
    "  '''\n",
    "  Args:\n",
    "    output_vocab_size: (int) Size of target vocabulary\n",
    "    embed_dim: (int) Embedding dimensions\n",
    "    hidden_size: (int) Dimensions of hidden state\n",
    "    max_length_encoder: (int) Maximum length of encoder sequence\n",
    "    dropout_value: (float) Value between 0 & 1\n",
    "    num_layers: (int) Number of stacked GRU layers, default is 1\n",
    "  ''' \n",
    "\n",
    "  def __init__(self, output_vocab_size, embed_dim, hidden_size, max_length_encoder, dropout_value, num_layers=1):\n",
    "      super(AttentionDecoder,self).__init__()\n",
    "   \n",
    "      self.hidden_size = hidden_size\n",
    "      self.num_layers = num_layers\n",
    "      self.output_vocab_size = output_vocab_size\n",
    "      self.dropout_p = dropout_value\n",
    "      self.max_length_encoder = max_length_encoder\n",
    "      \n",
    "      self.embedding = nn.Embedding(output_vocab_size, embed_dim) \n",
    "      \n",
    "      self.attention_layer = nn.Linear(hidden_size*2, max_length_encoder)\n",
    "      self.attention_combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "      self.s_layer = nn.Linear(hidden_size, 1)\n",
    "      self.x_layer = nn.Linear(hidden_size, 1)\n",
    "      self.context_layer = nn.Linear(hidden_size, 1)\n",
    "      self.linear_pgen = nn.Linear(3, 1)\n",
    "\n",
    "      self.gru_layer = nn.GRU(embed_dim, hidden_size)\n",
    "      self.output_layer = nn.Linear(hidden_size, output_vocab_size)\n",
    "      self.dropout_layer = nn.Dropout(self.dropout_p)    \n",
    "\n",
    "  def forward(self,input_tens,prev_hidden_state,encoder_output):\n",
    "      \n",
    "      #input_tens = [batch size]\n",
    "      input_tens = input_tens.unsqueeze(0)\n",
    "      embedded_outputs = self.embedding(input_tens)  #.view(1,1,-1)\n",
    "      #input_tens = [1 x batch size]\n",
    "      #embedded_outputs = [1 x batch size x embed dim]\n",
    "\n",
    "      embeddings_dropout = self.dropout_layer(embedded_outputs)\n",
    "      #embeddings_dropout = [1 x batch size x embed dim]\n",
    "\n",
    "      #prev_hidden_state = [n_layers x batch size x hidden dim] = [1 x batch size x hidden dim]\n",
    "      attention_layer_output = self.attention_layer(torch.cat((embeddings_dropout[0],prev_hidden_state[0]),1)) #was 0th index before\n",
    "      #cat = [batch size x (embed dim + hidden dim)] = [batch size x 2*(hidden dim)]\n",
    "      #in our case emdedding dimension is going to be same as hidden dimension\n",
    "      #attention_layer_output = [batch size x max encoder length]\n",
    "\n",
    "      attention_weights = nn.functional.softmax(attention_layer_output,dim=1)\n",
    "      #attention_weights = [batch size x max encoder length]\n",
    "      \n",
    "      attention_applied = torch.bmm(attention_weights.unsqueeze(1),encoder_output.permute(1, 0, 2)) # .unsqueeze(0)\n",
    "      #attention_weights = [batch size x max encoder length], after unsqueezing in 1st dim ==> [batch size x 1 x max encoder length]\n",
    "      #encoder_output = [max encoder length x batch size x hidden dim], after permute ==> [batch size x max encoder length x hidden dim] \n",
    "      #attention_applied = [batch size x 1 x hidden dim]\n",
    "      \n",
    "      attention_applied = attention_applied.permute(1, 0, 2)\n",
    "      #attention_applied = [1 x batch size x hidden dim]\n",
    "\n",
    "      attention_combine_logits = self.attention_combine(torch.cat((embeddings_dropout[0],attention_applied[0]),1)).unsqueeze(0)  #since gru requires a batch dimension\n",
    "      #embeddings_dropout = [1 x batch size x embed dim]\n",
    "      #attention_applied = [1 x batch size x hidden dim]\n",
    "      #cat = [batch size x (embed dim + hidden dim)] = [batch size x 2*(hidden dim)]\n",
    "      #attention_combine_logits = [batch size x hidden dim], after unsqueezing in 0th dim ==> [1 x batch size x hidden dim]\n",
    "      \n",
    "      attention_combine_relu = nn.functional.relu(attention_combine_logits)\n",
    "      #attention_combine_relu = [1 x batch size x hidden dim]\n",
    "\n",
    "      s_output = self.s_layer(prev_hidden_state[0])\n",
    "      #prev_hidden_state = [n_layers x batch size x hidden dime]\n",
    "      #s_output = [batch size x 1]\n",
    "\n",
    "      x_output = self.x_layer(embeddings_dropout[0])\n",
    "      #embeddings_dropout = [seq length x batch size x embed dim]\n",
    "      #x_output = [batch size x 1] as (hidden dim = embed dim)\n",
    "      \n",
    "      context = torch.flatten(attention_applied)\n",
    "      #attention_applied = [1 x batch size x hidden dim]\n",
    "      #context = [batch size * hidden dim]\n",
    "\n",
    "      context_weights = self.context_layer(attention_applied)\n",
    "      #context_weights = [1 x batch size x 1]\n",
    "\n",
    "      sx = torch.cat((s_output[0],x_output[0]),0)\n",
    "      #sx = [1 x 2*(unit)]\n",
    "      sxc = torch.cat((sx,context_weights[0][0]),0)\n",
    "      #sxc = [1 x 3*(unit)]\n",
    "      linear_pgen = self.linear_pgen(sxc)\n",
    "      #linear_pgen = [1 x 1]\n",
    "      m = nn.Sigmoid()\n",
    "      pgen = m(linear_pgen)\n",
    "      #pgen = [1 x 1]\n",
    "\n",
    "      output,hidden = self.gru_layer(attention_combine_relu,prev_hidden_state)\n",
    "      #attention_combine_relu = [1 x batch size x hidden dim]\n",
    "      #prev_hidden_state = [n_layers x batch size x hidden dime]\n",
    "      #output = [1 x batch size x hidden dim]\n",
    "      #hidden = [n_layers x batch size x hidden dime]\n",
    "\n",
    "      output_logits = self.output_layer(output)\n",
    "      #output_logits = [1 x batch size x output vocab size]\n",
    "      output_softmax = nn.functional.log_softmax(output_logits[0],dim=1)\n",
    "      #output_softmax = [batch size x output vocab size], softmax applied distribution over whole target vocab\n",
    "      return output_softmax,hidden,attention_weights,pgen\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return torch.zeros(1,1,self.hidden_size,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, attr, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #attr = [src len x batch size] (attr length = source length)\n",
    "        #src = [src len x batch size]\n",
    "        #trg = [trg len x batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        input_length = src.shape[0]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_vocab_size\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        init_hidden = torch.zeros(self.encoder.num_layers, batch_size, self.encoder.hidden_size).to(self.device) #(new)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        #hidden, cell = self.encoder(src)  (original)\n",
    "        enc_output, enc_hidden = self.encoder(src, attr, init_hidden) #(new)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        dec_input = trg[0,:]\n",
    "\n",
    "        duplicate_words = []\n",
    "        input_list = src.permute(1,0).tolist()\n",
    "        i = 0\n",
    "        for batch in input_list:\n",
    "            j = 0\n",
    "            for input_word in batch:\n",
    "                if id2word_cell[input_word] in word2id_cap.keys():\n",
    "                    duplicate_words.append( (i,j,word2id_cap[id2word_cell[input_word]]) )\n",
    "                j += 1\n",
    "            i += 1\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            #output, hidden, cell = self.decoder(dec_input, hidden, cell) (original)\n",
    "            decoder_output,decoder_hidden,decoder_attention,pgen = self.decoder(dec_input, enc_hidden, enc_output)\n",
    "            \n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            #top1 = decoder_output.argmax(1) (original)\n",
    "            \n",
    "            '''experiment with pgen'''\n",
    "            \n",
    "            P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "            #P_over_extended_vocab = [batch size x output vocab size] (exp(decoder_output)*pgen)\n",
    "\n",
    "            decoder_attention = decoder_attention  #.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "            #restricting decoder attention upto only input length\n",
    "            #decoder_attention = [batch size x input_length]\n",
    "            p_duplicate_list = torch.zeros([batch_size, input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "            #p_duplicate_list = [batch size x input_length x output vocab size] \n",
    "\n",
    "            p_duplicate_list = p_duplicate_list.tolist()\n",
    "            for (batch_id, duplicate_word_key,duplicate_word_value) in duplicate_words:\n",
    "                p_duplicate_list[batch_id][duplicate_word_key][duplicate_word_value] = 1 #making duplicate key,vals apparent\n",
    "\n",
    "            p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "            p_diag = torch.mm(decoder_attention.unsqueeze(1), p_duplicate)\n",
    "            #decoder_attention after unsqueezing ==> [batch size x 1 x input_length]\n",
    "            #p_diag = [batch size x 1 x output vocab size]\n",
    "            \n",
    "            p_diag = p_diag.squeeze(1)\n",
    "\n",
    "            p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "            #p_diag = p_diag*(1 - pgen)\n",
    "            \n",
    "            p_add_diag = []\n",
    "            for i in range(batch_size):\n",
    "                diag = torch.diag(p_diag[i],diagonal=0)\n",
    "                p_add_diag.append(diag.tolist())\n",
    "            \n",
    "            p_add_diag = torch.tensor(p_add_diag, dtype=torch.float, device=device) #new\n",
    "            #p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0) #p_diag.squeeze(0) ==> [output vocab size]\n",
    "            #p_add_diag = [batch size x output vocab size x output vocab size] \n",
    "\n",
    "            P_over_extended_vocab = torch.mm(P_over_extended_vocab.unsqueeze(1),p_add_diag).add(P_over_extended_vocab.unsqueeze(1))\n",
    "            #mm = [batch size x 1 x output vocab size]\n",
    "            #P_over_extended_vocab = [batch size x 1 x output vocab size] (element wise summation)\n",
    "            \n",
    "            P_over_extended_vocab = P_over_extended_vocab.squeeze(1)\n",
    "            '''for batch_id in range(batch_size):\n",
    "                for i in range(input_length):\n",
    "                    if not (1 in p_duplicate_list[batch_id][i]):\n",
    "                        P_over_extended_vocab[batch_id] = torch.cat((P_over_extended_vocab[batch_id], \n",
    "                                                           torch.mm(decoder_attention[batch_id][i].unsqueeze(0).unsqueeze(0), \n",
    "                                                                    torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0)'''\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = P_over_extended_vocab   # decoder_output was before           \n",
    "            \n",
    "            top1 = P_over_extended_vocab.argmax(1)\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            dec_input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4466]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diag = torch.randn(2, 3)\n",
    "p_add_diag = []\n",
    "for i in range(2):\n",
    "    diag = torch.diag(p_diag[i],diagonal=0)\n",
    "    p_add_diag.append(diag.tolist())\n",
    "\n",
    "p_add_diag = torch.tensor(p_add_diag, dtype=torch.float, device=device)\n",
    "\n",
    "p_diag[0][0].unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1650,  1.8783,  0.0852],\n",
       "        [ 0.2867,  0.1171,  0.2655]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diag = torch.randn(2, 3)\n",
    "p_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1255])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep = torch.randn(1)\n",
    "rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 0.  Target sizes: [3].  Tensor sizes: [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-fa6c6f740207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp_diag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_diag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mp_diag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 0.  Target sizes: [3].  Tensor sizes: [4]"
     ]
    }
   ],
   "source": [
    "p_diag[0] = torch.cat((p_diag[0], rep),0)\n",
    "p_diag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_DIM = len(ATTRIBUTE.vocab)\n",
    "INPUT_DIM = len(CELL.vocab)\n",
    "OUTPUT_DIM = len(CAPTION.vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "HID_DIM = 128\n",
    "MAX_CELL_LEN = max_source_length\n",
    "N_LAYERS = 1\n",
    "DEC_DROPOUT = 0.2\n",
    "\n",
    "enc = Encoder(attr_vocab_size=ATTR_DIM, \n",
    "              input_vocab_size=INPUT_DIM, \n",
    "              embed_size=ENC_EMB_DIM, \n",
    "              hidden_size=HID_DIM, \n",
    "              num_layers=1, \n",
    "              bidirectional=False)\n",
    "dec = AttentionDecoder(output_vocab_size=OUTPUT_DIM, \n",
    "                       embed_dim=DEC_EMB_DIM, \n",
    "                       hidden_size=HID_DIM, \n",
    "                       max_length_encoder=MAX_CELL_LEN, \n",
    "                       dropout_value=DEC_DROPOUT, \n",
    "                       num_layers=1)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(29688, 128)\n",
       "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (nonlinear): Tanh()\n",
       "    (gru_layer): GRU(128, 128)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (embedding): Embedding(25176, 128)\n",
       "    (attention_layer): Linear(in_features=256, out_features=12, bias=True)\n",
       "    (attention_combine): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (s_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (x_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (context_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (linear_pgen): Linear(in_features=3, out_features=1, bias=True)\n",
       "    (gru_layer): GRU(128, 128)\n",
       "    (output_layer): Linear(in_features=128, out_features=25176, bias=True)\n",
       "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,537,707 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTION_PAD_IDX = CAPTION.vocab.stoi[CAPTION.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=CAPTION_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        attr = batch.attributes\n",
    "        src = batch.cells\n",
    "        trg = batch.captions\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(attr, src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            attr = batch.attributes\n",
    "            src = batch.cells\n",
    "            trg = batch.captions\n",
    "\n",
    "            output = model(attr, src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "matrices expected, got 3D, 3D tensors at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\TH/generic/THTensorMath.cpp:36",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-9d31ed64a1e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-2eba3bf5347b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#trg = [trg len, batch size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-7c0c68dcea1f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, attr, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mp_duplicate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_duplicate_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0mp_diag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_attention\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_duplicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;31m#decoder_attention after unsqueezing ==> [batch size x 1 x input_length]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;31m#p_diag = [batch size x 1 x output vocab size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: matrices expected, got 3D, 3D tensors at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\TH/generic/THTensorMath.cpp:36"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of encoder vocab:  29687\n",
      "Size of decoder vocab: Full 25175 | Frequent 5565\n"
     ]
    }
   ],
   "source": [
    "print('Size of encoder vocab: ',len(word2Index_enc))\n",
    "print('Size of decoder vocab: Full {} | Frequent {}'.format(len(word2Index_dec_big),len(word2Index_dec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y40789vQM8gc"
   },
   "outputs": [],
   "source": [
    "def train(encoder, decoder, input_tensor, attr_tensor, target_tensor, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length, iters, \n",
    "          teacher_forcing_ratio = 0.5, clip = 0.4):\n",
    "  '''\n",
    "  Arg:\n",
    "    encoder: encoder model to train\n",
    "    decoder: decoder model to train\n",
    "    input_tensor: source seq in tensor form [seq length x 1] batch size = 1\n",
    "    target_tensor: target seq in tensor form [seq length x 1] batch size = 1\n",
    "    encoder_optimizer: optimizer for encoder\n",
    "    decoder_optimizer: optimizer for decoder\n",
    "    citerion: \n",
    "    max length: maximum source length \n",
    "    iters: number of iterations\n",
    "    teacher_forcing_ratio: if teacher forcing, actual next token is useed as next input\n",
    "    clip: to prevent gradients from exploding \n",
    "  '''\n",
    "  encoder_optimizer.zero_grad() #initialize encoder_optimizer at zero gradient\n",
    "  decoder_optimizer.zero_grad() #initialize decoder_optimizer at zero gradient\n",
    "\n",
    "  #prev_unk_word = ''\n",
    "  encoder_hidden = encoder.init_hidden()\n",
    "  #encoder_hidden = [1 x 1 x hidden dim]\n",
    "\n",
    "  encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "  #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "  input_length = input_tensor.size(0)\n",
    "  output_length = target_tensor.size(0)\n",
    "\n",
    "  loss = 0\n",
    "\n",
    "  for encoder_index in range(0, input_length):\n",
    "    encoder_output,encoder_hidden = encoder(input_tensor[encoder_index], attr_tensor[encoder_index], encoder_hidden)\n",
    "    #input_tensor[encoder_index] = [1 x 1 x embed dim] (embed dim = hidden dim)\n",
    "    #encoder_hidden = [1 x 1 x hidden dim] {encoder arg inp}\n",
    "    #encoder_hidden = [n_layers*n direction x 1 x hidden dime] {encoder product}\n",
    "    #encoder_output = [seq length x 1 x hidden dim*n direction]\n",
    "    #seq length, n_layers, n direction = 1  \n",
    "\n",
    "    encoder_outputs[encoder_index] = encoder_output[0,0] # [1 x hidden dim]\n",
    "    #encoder_outputs: [seq length x hidden dim] ==> [seq length x hidden dim] (hidden state from all 0 to new)\n",
    "\n",
    "  decoder_input = torch.tensor([word2Index_dec['<START>']],device=device)\n",
    "  #decoder_input = [1 x 1]\n",
    "  decoder_hidden = encoder_hidden\n",
    "  #decoder_hidden = [1 x 1 x hidden dim]\n",
    "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "  extended_vocab = psuInd2Word_dec.copy()\n",
    "  reverse_extended_vocab = word2PsuInd_dec.copy()\n",
    "  duplicate_words = {}\n",
    "  extend_key = len(word2Index_dec.keys())\n",
    "  input_list = input_tensor.tolist()\n",
    "  i =0\n",
    "  for input_word in input_list:\n",
    "    if ind2Word_enc[input_word[0]] in word2Index_dec.keys():\n",
    "      duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word[0]]]\n",
    "    else:\n",
    "      extended_vocab[extend_key] = ind2Word_enc[input_word[0]]\n",
    "      reverse_extended_vocab[ind2Word_enc[input_word[0]]] = extend_key\n",
    "      extend_key += 1\n",
    "    i = i+1\n",
    "\n",
    "  if use_teacher_forcing:\n",
    "    for decoder_index in range(output_length):\n",
    "      decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "      #decoder_input = [1 x 1]\n",
    "      #decoder_hidden = [1 x 1 x hidden dim]\n",
    "      #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "      #decoder_output = [1 x output vocab size]\n",
    "      #decoder_hidden = [1 x 1 x hidden dime]\n",
    "      #decoder_attention = [1 x seq length]\n",
    "      #pgen = [1 x 1]\n",
    "\n",
    "      P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "      #P_over_extended_vocab = [1 x output vocab size] (exp(decoder_output)*pgen)\n",
    "\n",
    "      decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "      #restricting decoder attention upto only input length\n",
    "      #decoder_attention = [1 x input_length]\n",
    "      p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "      #p_duplicate_list = [input_length x output vocab size] \n",
    "\n",
    "      p_duplicate_list = p_duplicate_list.tolist()\n",
    "      for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "        p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1 #making duplicate key,vals apparent\n",
    "      \n",
    "      p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "      p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "      #p_diag = [1 x output vocab size]\n",
    "      \n",
    "      p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "      #p_diag = p_diag*(1 - pgen)\n",
    "\n",
    "      p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0) #p_diag.squeeze(0) ==> [output vocab size]\n",
    "      #p_add_diag = [output vocab size x output vocab size]\n",
    "\n",
    "      P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "      #mm = [1 x output vocab size]\n",
    "      #P_over_extended_vocab = [1 x output vocab size] (element wise summation)\n",
    "\n",
    "      for i in range(input_length):\n",
    "        if not (1 in p_duplicate_list[i]):\n",
    "          P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "\n",
    "      try:\n",
    "        loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
    "        loss.backward(retain_graph=True)\n",
    "      except KeyError:\n",
    "        loss += torch.tensor(0,dtype=torch.float,device=device)\n",
    "      decoder_input = target_tensor[decoder_index]\n",
    "  else:\n",
    "\n",
    "    for decoder_index in range(output_length):\n",
    "      decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs) \n",
    "      P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "\n",
    "      decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "      p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "      p_duplicate_list = p_duplicate_list.tolist()\n",
    "      for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "        p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1\n",
    "      p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "      p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "      p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "      p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0)\n",
    "      P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "\n",
    "      for i in range(input_length):\n",
    "        if not (1 in p_duplicate_list[i]):\n",
    "          P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "\n",
    "      try:\n",
    "        loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
    "        loss.backward(retain_graph=True)\n",
    "      except KeyError:\n",
    "        loss += torch.tensor(0,dtype=torch.float,device=device)\n",
    "      idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
    "      if idx.item() < len(word2Index_dec_big.keys()):   \n",
    "        decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
    "      elif idx.item() >= len(word2Index_dec_big.keys()):\n",
    "        #prev_unk_word = extended_vocab[idx.item()] # use <UNK> if doesn't work\n",
    "        decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
    "      \n",
    "      if (decoder_input.item() == word2Index_dec['<END>']):\n",
    "        break\n",
    "\n",
    "  if iters > 20000:\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()/output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE4KJmThZd5C"
   },
   "outputs": [],
   "source": [
    "def validate(encoder, decoder, input_tensor, attr_tensor, target_tensor, criterion, max_length):\n",
    "  '''\n",
    "  Arg:\n",
    "    encoder: encoder model trained\n",
    "    decoder: decoder model trained\n",
    "    input_tensor: source seq in tensor form [seq length x 1 x embed dim] batch size = 1\n",
    "    target_tensor: target seq in tensor form [seq length x 1 x embed dim] batch size = 1\n",
    "    citerion: \n",
    "    max length: maximum target length desired\n",
    "  '''\n",
    "  with torch.no_grad():\n",
    "    \n",
    "    #prev_unk_word = ''\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    #encoder_hidden = [1 x 1 x hidden dim]\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device = device)\n",
    "    #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for encoder_index in range(0, input_length):\n",
    "      encoder_output,encoder_hidden = encoder(input_tensor[encoder_index], attr_tensor[encoder_index], encoder_hidden)\n",
    "      #input_tensor[encoder_index] = [1 x 1 x embed dim] (embed dim = hidden dim)\n",
    "      #encoder_hidden = [1 x 1 x hidden dim] {encoder arg inp}\n",
    "      #encoder_hidden = [n_layers*n direction x 1 x hidden dime] {encoder product}\n",
    "      #encoder_output = [seq length x 1 x hidden dim*n direction]\n",
    "      #seq length, n_layers, n direction = 1  \n",
    "\n",
    "      encoder_outputs[encoder_index] = encoder_output[0,0] # [1 x hidden dim]\n",
    "      #encoder_outputs: [seq length x hidden dim] ==> [seq length x hidden dim] (hidden state from all 0 to new)\n",
    "\n",
    "    decoder_input = torch.tensor([word2Index_dec['<START>']],device=device)   \n",
    "    #decoder_input = [1 x 1]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #decoder_hidden = [1 x 1 x hidden dim]\n",
    "\n",
    "    extended_vocab = psuInd2Word_dec.copy()\n",
    "    reverse_extended_vocab = word2PsuInd_dec.copy()\n",
    "    duplicate_words = {}\n",
    "    extend_key = len(word2Index_dec.keys())\n",
    "    input_list = input_tensor.tolist()\n",
    "    i =0\n",
    "    for input_word in input_list:\n",
    "      if ind2Word_enc[input_word[0]] in word2Index_dec.keys():\n",
    "        duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word[0]]]\n",
    "      else:\n",
    "        extended_vocab[extend_key] = ind2Word_enc[input_word[0]]\n",
    "        reverse_extended_vocab[ind2Word_enc[input_word[0]]] = extend_key\n",
    "        extend_key += 1\n",
    "      i = i+1\n",
    "\n",
    "    for decoder_index in range(output_length):\n",
    "      decoder_output,decoder_hidden,decoder_attention,pgen = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
    "      #decoder_input = [1 x 1]\n",
    "      #decoder_hidden = [1 x 1 x hidden dim]\n",
    "      #encoder_outputs = [seq length x hidden dim]\n",
    "\n",
    "      #decoder_output = [1 x output vocab size]\n",
    "      #decoder_hidden = [1 x 1 x hidden dime]\n",
    "      #decoder_attention = [1 x seq length]\n",
    "      #pgen = [1 x 1]\n",
    "\n",
    "      P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "      #P_over_extended_vocab = [1 x output vocab size] (exp(decoder_output)*pgen)\n",
    "\n",
    "      decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "      #restricting decoder attention upto only input length\n",
    "      #decoder_attention = [1 x input_length]\n",
    "      p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "      #p_duplicate_list = [input_length x output vocab size] \n",
    "\n",
    "      p_duplicate_list = p_duplicate_list.tolist()\n",
    "      for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "        p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1 #making duplicate key,vals apparent\n",
    "      \n",
    "      p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "      p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "      #p_diag = [1 x output vocab size]\n",
    "      \n",
    "      p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "      #p_diag = p_diag*(1 - pgen)\n",
    "\n",
    "      p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0) #p_diag.squeeze(0) ==> [output vocab size]\n",
    "      #p_add_diag = [output vocab size x output vocab size]\n",
    "\n",
    "      P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "      #mm = [1 x output vocab size]\n",
    "      #P_over_extended_vocab = [1 x output vocab size] (element wise summation)\n",
    "\n",
    "      for i in range(input_length):\n",
    "        if not (1 in p_duplicate_list[i]):\n",
    "          P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], \n",
    "                                              torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), \n",
    "                                                      torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "\n",
    "      try:\n",
    "        loss += -torch.log(P_over_extended_vocab[0][ reverse_extended_vocab[ ind2Word_dec_big[ target_tensor[decoder_index].item() ] ] ] + 1e-12)\n",
    "      except KeyError:\n",
    "        loss += torch.tensor(0,dtype=torch.float,device=device)\n",
    "      idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
    "      if idx.item() < len(word2Index_dec_big.keys()):   \n",
    "        decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
    "      elif idx.item() >= len(word2Index_dec_big.keys()):\n",
    "        #prev_unk_word = extended_vocab[idx.item()]\n",
    "        decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
    "      if (decoder_input.item() == word2Index_dec['<END>']):\n",
    "        break\n",
    "\n",
    "  return loss.item()/output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocM_vNF6KTWW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    if percent != 0:\n",
    "      es = s / (percent)\n",
    "      rs = es - s\n",
    "      return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVoWvRycXU2T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('checkpoints_luong/encoder'):\n",
    "    os.makedirs('checkpoints_luong/encoder')\n",
    "if not os.path.exists('checkpoints_luong/decoder'):\n",
    "    os.makedirs('checkpoints_luong/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBToTQxGLXgI"
   },
   "outputs": [],
   "source": [
    "# Dictionary for creating loss graph\n",
    "loss_graph = {}\n",
    "\n",
    "def train_Iters(encoder,decoder,n_iters, print_every=1, plot_every=5,learning_rate = 0.0005):\n",
    "  # start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0  # Reset every print_every\n",
    "  plot_loss_total = 0\n",
    "\n",
    "  print_val_loss = 0\n",
    "\n",
    "  encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "  \n",
    "  train_attr, train_cell, train_cap = attr_cell_caption_split(train_file)\n",
    "  attr_input = [[word2Index_att[word] if word in word2Index_att.keys() else word2Index_att['<UNK>'] for word in sentence.split()] for sentence in train_attr ]\n",
    "  encoder_input = [[word2Index_enc[word] if word in word2Index_enc.keys() else word2Index_enc['<UNK>'] for word in sentence.split()] for sentence in train_cell ]\n",
    "  decoder_input = [[word2Index_dec_big[word] if word in word2Index_dec_big.keys() else word2Index_dec_big['<UNK>'] for word in sentence.split()] for sentence in train_cap ]\n",
    "  train_pairs = [[attr,enc,dec] for attr,enc,dec in zip(attr_input,encoder_input,decoder_input)]\n",
    "  training_pairs = [random.choice(train_pairs) for i in range(n_iters)]\n",
    "    \n",
    "  \n",
    "  val_attr, val_cell, val_cap = attr_cell_caption_split(val_file)\n",
    "  attr_val = [[word2Index_att[word] if word in word2Index_att.keys() else word2Index_att['<UNK>'] for word in sentence.split()] for sentence in val_attr ]\n",
    "  encoder_val = [[word2Index_enc[word] if word in word2Index_enc.keys() else word2Index_enc['<UNK>'] for word in sentence.split()] for sentence in val_cell ]\n",
    "  decoder_val = [[word2Index_dec_big[word] if word in word2Index_dec_big.keys() else word2Index_dec_big['<UNK>'] for word in sentence.split()] for sentence in val_cap ]\n",
    "  val_pairs = [[attr,enc,dec] for attr,enc,dec in zip(attr_val,encoder_val,decoder_val)]\n",
    "  validation_pairs = [random.choice(val_pairs) for i in range(n_iters)]\n",
    "\n",
    "  criterion = nn.NLLLoss()\n",
    "  for iters in range(n_iters):\n",
    "    training_pair = training_pairs[iters - 1]\n",
    "    attr_tensor = training_pair[0]\n",
    "    input_tensor = training_pair[1]\n",
    "    target_tensor = training_pair[2]\n",
    "\n",
    "    attr_tensor = torch.tensor(attr_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "    input_tensor = torch.tensor(input_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "    target_tensor = torch.tensor(target_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "\n",
    "    loss = train(encoder,decoder,input_tensor,attr_tensor,target_tensor,\n",
    "                 encoder_optimizer,decoder_optimizer,criterion,max_source_length, iters=n_iters)\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    validation_pair = validation_pairs[iters - 1]\n",
    "    val_attr_tensor = validation_pair[0]\n",
    "    val_input_tensor = validation_pair[1]\n",
    "    val_target_tensor = validation_pair[2]\n",
    "\n",
    "    val_attr_tensor = torch.tensor(val_attr_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "    val_input_tensor = torch.tensor(val_input_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "    val_target_tensor = torch.tensor(val_target_tensor, dtype=torch.long, device = device).view(-1, 1)\n",
    "\n",
    "    val_loss = validate(encoder, decoder, val_input_tensor,val_attr_tensor, val_target_tensor, criterion, max_source_length)\n",
    "    print_val_loss += val_loss\n",
    "\n",
    "    '''if iters % print_every == 0:\n",
    "        torch.save(encoder, 'checkpoints_luong/encoder/encoder_{}.pt'.format(iters))\n",
    "        torch.save(decoder, 'checkpoints_luong/decoder/decoder_{}.pt'.format(iters))'''\n",
    "    \n",
    "    if iters % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        val_loss_avg = print_val_loss / print_every\n",
    "        print_val_loss = 0\n",
    "\n",
    "        print('Iteration: {}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(iters, print_loss_avg, val_loss_avg)) # iters / len(arr) * 100, before\n",
    "        evaluateRandomly(rnn_encoder, rnn_decoder, train_attr, train_cell, train_cap)\n",
    "        if iters > 0:\n",
    "          loss_graph[iters] = print_loss_avg\n",
    "\n",
    "    if iters % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "  # showPlot(plot_losses)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAJAoFoJDLrm"
   },
   "outputs": [],
   "source": [
    "hidden_size = embed_dim = 128\n",
    "rnn_encoder = Encoder(len(word2Index_att.keys()),len(word2Index_enc.keys()),embed_dim,hidden_size).to(device=device)\n",
    "rnn_decoder = AttentionDecoder(len(word2Index_dec_big.keys()),embed_dim,hidden_size,max_source_length,0.2).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-njk60qXU31"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, attr_seqs=None, source_seqs=None, target_seqs=None, n=1):\n",
    "    for i in range(n):\n",
    "        idx = random.choice(range(len(source_seqs)))\n",
    "        source_seq,attr_seq = source_seqs[idx], attr_seqs[idx]\n",
    "        attr_inp = [word2Index_att[word] if word in word2Index_att.keys() else word2Index_att['<UNK>'] for word in attr_seq.split()]\n",
    "        source_inp = [word2Index_enc[word] if word in word2Index_enc.keys() else word2Index_enc['<UNK>'] for word in source_seq.split()]\n",
    "        '''\n",
    "        len_enc_vocab = len(word2Index_enc)\n",
    "        source_inp = list()\n",
    "        for word in source_seq.split():\n",
    "            i = 0\n",
    "            if word in word2Index_enc.keys():\n",
    "                source_inp.append(word2Index_enc[word])\n",
    "            else:\n",
    "                word2Index_enc[word] = len_enc_vocab + i\n",
    "                ind2Word_enc[len_enc_vocab + i] = word\n",
    "                i += 1\n",
    "                source_inp.append(word2Index_enc[word])\n",
    "         '''       \n",
    "        source_tensor = torch.tensor(source_inp,dtype=torch.long,device=device)\n",
    "        output_words, attentions = evaluate(encoder, decoder, source_tensor)\n",
    "        output_seq = ' '.join(output_words)\n",
    "        print('ATTRIBUTE: ',attr_seq)\n",
    "        print('   SOURCE: ',source_seq)\n",
    "        if target_seqs is not None:\n",
    "            target_seq = target_seqs[idx]\n",
    "            print('   ACTUAL: ',target_seq)\n",
    "            #target_out = [[word2Index_dec_big[word] if word in word2Index_dec_big.keys() else word2Index_dec_big['<UNK>'] for word in target_seq.split()]]\n",
    "        print('PREDICTED: ',output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ObARxnAyoUTA"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, encoder_tensor, attr_tensor,\n",
    "             max_source_length=max_source_length, max_summary_length=max_summary_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = encoder_tensor\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "\n",
    "        #prev_unk_word = ''\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_source_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei].unsqueeze(0),attr_tensor[ei].unsqueeze(0),\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        extended_vocab = psuInd2Word_dec.copy()\n",
    "        duplicate_words = {}\n",
    "        extend_key = len(word2Index_dec.keys())\n",
    "        input_list = input_tensor.tolist()\n",
    "        i =0\n",
    "        for input_word in input_list:\n",
    "          if ind2Word_enc[input_word] in word2Index_dec.keys():\n",
    "            duplicate_words[i] = word2PsuInd_dec[ind2Word_enc[input_word]]\n",
    "          else:\n",
    "            extended_vocab[extend_key] = ind2Word_enc[input_word]\n",
    "            extend_key += 1\n",
    "          i = i+1\n",
    "\n",
    "        decoder_input = torch.tensor([word2Index_dec['<START>']], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_summary_length, max_source_length)\n",
    "\n",
    "        for di in range(max_summary_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention,pgen = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "\n",
    "            P_over_extended_vocab = torch.exp(decoder_output)*pgen.expand_as(torch.exp(decoder_output))\n",
    "\n",
    "            decoder_attention = decoder_attention.squeeze(0)[0:input_length].unsqueeze(0)\n",
    "            p_duplicate_list = torch.zeros([input_length, P_over_extended_vocab.size(1)], device=device)\n",
    "            p_duplicate_list = p_duplicate_list.tolist()\n",
    "            for (duplicate_word_key,duplicate_word_value) in duplicate_words.items():\n",
    "              p_duplicate_list[duplicate_word_key][duplicate_word_value] = 1\n",
    "            p_duplicate = torch.tensor(p_duplicate_list, dtype=torch.float, device=device)\n",
    "            p_diag = torch.mm(decoder_attention, p_duplicate)\n",
    "            p_diag = p_diag*(torch.tensor([1], device=device).sub(pgen)).expand_as(p_diag)\n",
    "            p_add_diag = torch.diag(p_diag.squeeze(0),diagonal=0)\n",
    "            P_over_extended_vocab = torch.mm(P_over_extended_vocab,p_add_diag).add(P_over_extended_vocab)\n",
    "\n",
    "            for i in range(input_length):\n",
    "              if not (1 in p_duplicate_list[i]):\n",
    "                P_over_extended_vocab = torch.cat((P_over_extended_vocab[0], torch.mm(decoder_attention.squeeze(0)[i].unsqueeze(0).unsqueeze(0), torch.tensor([1], device=device).sub(pgen).unsqueeze(0)).squeeze(0)),0).unsqueeze(0)\n",
    "\n",
    "            idx = torch.topk(P_over_extended_vocab, k=1, dim=1)[1]\n",
    "            if idx.item() < len(word2Index_dec_big.keys()):   \n",
    "              decoder_input = torch.tensor([idx.item()],dtype=torch.long,device=device)\n",
    "              decoded_words.append(extended_vocab[idx.item()])\n",
    "            elif idx.item() >= len(word2Index_dec_big.keys()):\n",
    "              decoder_input = torch.tensor([0],dtype=torch.long,device=device)\n",
    "              #prev_unk_word = extended_vocab[idx.item()] \n",
    "              decoded_words.append('<UNK>')\n",
    "            if idx.item() == word2Index_dec['<END>']:\n",
    "              decoded_words.append('<END>')\n",
    "              break\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oYYHCcO04vV5",
    "outputId": "89a3a213-5372-417c-dfac-d70ee4182139"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2 x 128], m2: [256 x 128] at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\TH/generic/THTensorMath.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d7829ccb055e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_Iters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_encoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrnn_decoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-72600effe935>\u001b[0m in \u001b[0;36mtrain_Iters\u001b[1;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     loss = train(encoder,decoder,input_tensor,attr_tensor,target_tensor,\n\u001b[1;32m---> 42\u001b[1;33m                  encoder_optimizer,decoder_optimizer,criterion,max_source_length, iters=n_iters)\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mplot_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-08a74c473d39>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(encoder, decoder, input_tensor, attr_tensor, target_tensor, encoder_optimizer, decoder_optimizer, criterion, max_length, iters, teacher_forcing_ratio, clip)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mencoder_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mencoder_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mencoder_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;31m#input_tensor[encoder_index] = [1 x 1 x embed dim] (embed dim = hidden dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#encoder_hidden = [1 x 1 x hidden dim] {encoder arg inp}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-fbfc3c877dc0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_cell, input_attr, prev_hidden_state)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#input_source = [seq length x batch size x 2*embed dim]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0minput_tanh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_source\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0membedded_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tanh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1611\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1612\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [2 x 128], m2: [256 x 128] at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\TH/generic/THTensorMath.cpp:41"
     ]
    }
   ],
   "source": [
    "train_Iters(rnn_encoder,rnn_decoder,100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PointerGenerator_1stRun.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
