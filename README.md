## Attentive-Seq2Seq-with-Copying-Mechanism-for-Table-to-text
Model has been built in Pytorch, check `requirement.txt` file for dependencies.
Seq2seq with attention and copying mechanism has been deployed to attend to relevant words from source sequence while decoding.
The copying mechanism has been proven effective in abstractive summarization tasks (See et al., 2017) and in previous
works on table-to-text generation (Bao et al., 2018). 

## Datasets
Model has been testet on small fabricated datasets which is provided in 'Datasets' folder to play around with. For thorough testing
and future improvements of our model, Wikitable dataset (Bao et al., 2018), both in csv and original foramt is also provided to check the model's performance on large 
tabular dataset.

### For more info and to get the clearer understanding of the model, check out the ipynb files.
